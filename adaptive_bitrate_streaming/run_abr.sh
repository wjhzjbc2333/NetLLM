python run_abr.py --adapt \
                  --test \
                  --frozen \
                  --state-use-self-attention \
                  --grad-accum-steps 32 \
                  --seed 666 \
                  --plm-type llama \
                  --plm-size large \
                  --rank 128 \
                  --device cuda:0 \
                  --state-feature-dim 256 \
                  --w 20 \
                  --gamma 1. \
                  --lr 0.0001 \
                  --warmup-steps 2000 \
                  --num-epochs 50 \
                  --eval-per-epoch 2 \
                  --target-return-scale 1 \
                  --save-checkpoint-per-epoch 20 \
                  --state-attn-hidden-dim 1024