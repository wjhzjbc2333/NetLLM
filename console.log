Learning rate schedule: Cosine Annealing with Warmup (from transformers)
  Dataset size: 996
  Steps per epoch: 32
  Total training steps: 9600
  Warmup steps: 1000
Step 0 - mean train loss  3.281984
Step 100 - mean train loss  2.615209
Step 200 - mean train loss  2.596666
Step 300 - mean train loss  2.593826
Step 400 - mean train loss  2.590799
Step 500 - mean train loss  2.596453
Step 600 - mean train loss  2.583587
Step 700 - mean train loss  2.582929
Step 800 - mean train loss  2.579977
Step 900 - mean train loss  2.574546
Epoch 0 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #0 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.17654514312744,
 'training/train_loss_mean': np.float64(2.570093580757279),
 'training/train_loss_std': np.float64(0.3536678074551181)}
Step 0 - mean train loss  3.192577
Step 100 - mean train loss  2.522838
Step 200 - mean train loss  2.502089
Step 300 - mean train loss  2.519708
Step 400 - mean train loss  2.529496
Step 500 - mean train loss  2.516431
Step 600 - mean train loss  2.509485
Step 700 - mean train loss  2.503889
Step 800 - mean train loss  2.494700
Step 900 - mean train loss  2.484710
Epoch 1 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #1 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.45913577079773,
 'training/train_loss_mean': np.float64(2.4788700220096542),
 'training/train_loss_std': np.float64(0.34847067597859444)}
Step 0 - mean train loss  2.885643
Step 100 - mean train loss  2.448162
Step 200 - mean train loss  2.394323
Step 300 - mean train loss  2.384038
Step 400 - mean train loss  2.359822
Step 500 - mean train loss  2.338184
Step 600 - mean train loss  2.321725
Step 700 - mean train loss  2.308378
Step 800 - mean train loss  2.295038
Step 900 - mean train loss  2.282186
Epoch 2 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #2 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.29708576202393,
 'training/train_loss_mean': np.float64(2.2738002762497667),
 'training/train_loss_std': np.float64(0.42089410343104633)}
Step 0 - mean train loss  1.337013
Step 100 - mean train loss  2.165029
Step 200 - mean train loss  2.129775
Step 300 - mean train loss  2.127042
Step 400 - mean train loss  2.131115
Step 500 - mean train loss  2.119896
Step 600 - mean train loss  2.126625
Step 700 - mean train loss  2.127371
Step 800 - mean train loss  2.125011
Step 900 - mean train loss  2.109292
Epoch 3 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #3 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.83378052711487,
 'training/train_loss_mean': np.float64(2.1019827731881278),
 'training/train_loss_std': np.float64(0.7647978756565963)}
Step 0 - mean train loss  3.361722
Step 100 - mean train loss  2.040477
Step 200 - mean train loss  2.007966
Step 300 - mean train loss  2.057974
Step 400 - mean train loss  2.047914
Step 500 - mean train loss  2.024071
Step 600 - mean train loss  2.016859
Step 700 - mean train loss  2.007296
Step 800 - mean train loss  1.993396
Step 900 - mean train loss  1.987727
Epoch 4 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #4 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.3084647655487,
 'training/train_loss_mean': np.float64(1.9892904098373818),
 'training/train_loss_std': np.float64(0.8470606591941571)}
Step 0 - mean train loss  0.851350
Step 100 - mean train loss  1.980072
Step 200 - mean train loss  1.860258
Step 300 - mean train loss  1.866094
Step 400 - mean train loss  1.863008
Step 500 - mean train loss  1.839323
Step 600 - mean train loss  1.839350
Step 700 - mean train loss  1.841013
Step 800 - mean train loss  1.822364
Step 900 - mean train loss  1.816444
Epoch 5 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #5 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.30879998207092,
 'training/train_loss_mean': np.float64(1.806339798562498),
 'training/train_loss_std': np.float64(0.743087443276846)}
Step 0 - mean train loss  1.756737
Step 100 - mean train loss  1.638749
Step 200 - mean train loss  1.657661
Step 300 - mean train loss  1.699299
Step 400 - mean train loss  1.683614
Step 500 - mean train loss  1.661686
Step 600 - mean train loss  1.658034
Step 700 - mean train loss  1.666500
Step 800 - mean train loss  1.661131
Step 900 - mean train loss  1.666080
Epoch 6 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #6 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.28532552719116,
 'training/train_loss_mean': np.float64(1.6808185028024467),
 'training/train_loss_std': np.float64(0.7745734568180688)}
Step 0 - mean train loss  2.355837
Step 100 - mean train loss  1.646742
Step 200 - mean train loss  1.639879
Step 300 - mean train loss  1.622411
Step 400 - mean train loss  1.577409
Step 500 - mean train loss  1.591097
Step 600 - mean train loss  1.576892
Step 700 - mean train loss  1.567635
Step 800 - mean train loss  1.573556
Step 900 - mean train loss  1.558444
Epoch 7 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #7 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.7187135219574,
 'training/train_loss_mean': np.float64(1.559198573171853),
 'training/train_loss_std': np.float64(0.7585437511096172)}
Step 0 - mean train loss  1.743842
Step 100 - mean train loss  1.500243
Step 200 - mean train loss  1.484535
Step 300 - mean train loss  1.476792
Step 400 - mean train loss  1.458037
Step 500 - mean train loss  1.458438
Step 600 - mean train loss  1.459782
Step 700 - mean train loss  1.462135
Step 800 - mean train loss  1.464685
Step 900 - mean train loss  1.463277
Epoch 8 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #8 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.2351005077362,
 'training/train_loss_mean': np.float64(1.481035928679518),
 'training/train_loss_std': np.float64(0.735755573190671)}
Step 0 - mean train loss  0.530354
Step 100 - mean train loss  1.701807
Step 200 - mean train loss  1.574595
Step 300 - mean train loss  1.520424
Step 400 - mean train loss  1.536592
Step 500 - mean train loss  1.536440
Step 600 - mean train loss  1.538444
Step 700 - mean train loss  1.538687
Step 800 - mean train loss  1.532831
Step 900 - mean train loss  1.530663
Epoch 9 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #9 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.40794730186462,
 'training/train_loss_mean': np.float64(1.5346071674342614),
 'training/train_loss_std': np.float64(0.8124098197486964)}
Step 0 - mean train loss  0.981064
Step 100 - mean train loss  1.566995
Step 200 - mean train loss  1.608533
Step 300 - mean train loss  1.526450
Step 400 - mean train loss  1.533106
Step 500 - mean train loss  1.542494
Step 600 - mean train loss  1.515643
Step 700 - mean train loss  1.516955
Step 800 - mean train loss  1.505686
Step 900 - mean train loss  1.490645
Epoch 10 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #10 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.22250771522522,
 'training/train_loss_mean': np.float64(1.4813011324549297),
 'training/train_loss_std': np.float64(0.7425681947478471)}
Step 0 - mean train loss  1.377659
Step 100 - mean train loss  1.435390
Step 200 - mean train loss  1.390427
Step 300 - mean train loss  1.415332
Step 400 - mean train loss  1.414610
Step 500 - mean train loss  1.423435
Step 600 - mean train loss  1.431163
Step 700 - mean train loss  1.413700
Step 800 - mean train loss  1.429441
Step 900 - mean train loss  1.422772
Epoch 11 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #11 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.36196255683899,
 'training/train_loss_mean': np.float64(1.4142773670245365),
 'training/train_loss_std': np.float64(0.6652272321935995)}
Step 0 - mean train loss  1.101892
Step 100 - mean train loss  1.309906
Step 200 - mean train loss  1.360938
Step 300 - mean train loss  1.389962
Step 400 - mean train loss  1.389626
Step 500 - mean train loss  1.398297
Step 600 - mean train loss  1.384723
Step 700 - mean train loss  1.368555
Step 800 - mean train loss  1.365698
Step 900 - mean train loss  1.340391
Epoch 12 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #12 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.95984315872192,
 'training/train_loss_mean': np.float64(1.3139165704060032),
 'training/train_loss_std': np.float64(0.6291383736988052)}
Best model saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/best_model
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.415135138988906),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.415135138988906),
 'mean_qoe': np.float64(0.0009393904551040225),
 'time/evaluation': 234.2763991355896,
 'total_qoe': np.float64(4.415135138988906)}
Step 0 - mean train loss  1.090384
Step 100 - mean train loss  1.032635
Step 200 - mean train loss  1.045620
Step 300 - mean train loss  1.012306
Step 400 - mean train loss  0.981817
Step 500 - mean train loss  0.958822
Step 600 - mean train loss  0.969551
Step 700 - mean train loss  0.968721
Step 800 - mean train loss  0.981775
Step 900 - mean train loss  0.958744
Epoch 13 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #13 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.04702067375183,
 'training/train_loss_mean': np.float64(0.9593667264833269),
 'training/train_loss_std': np.float64(0.4089240888689712)}
Step 0 - mean train loss  0.942021
Step 100 - mean train loss  1.022764
Step 200 - mean train loss  0.930500
Step 300 - mean train loss  0.874317
Step 400 - mean train loss  0.834132
Step 500 - mean train loss  0.836586
Step 600 - mean train loss  0.818386
Step 700 - mean train loss  0.792549
Step 800 - mean train loss  0.772497
Step 900 - mean train loss  0.769507
Epoch 14 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #14 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.82001304626465,
 'training/train_loss_mean': np.float64(0.7628974647459436),
 'training/train_loss_std': np.float64(0.40806560845008955)}
Best model saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/best_model
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.481963167359908),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.481963167359908),
 'mean_qoe': np.float64(0.0009536091845446612),
 'time/evaluation': 234.45504665374756,
 'total_qoe': np.float64(4.481963167359908)}
Step 0 - mean train loss  0.783008
Step 100 - mean train loss  0.675002
Step 200 - mean train loss  0.663187
Step 300 - mean train loss  0.643631
Step 400 - mean train loss  0.638015
Step 500 - mean train loss  0.642670
Step 600 - mean train loss  0.672087
Step 700 - mean train loss  0.668960
Step 800 - mean train loss  0.674260
Step 900 - mean train loss  0.688691
Epoch 15 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #15 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.86429858207703,
 'training/train_loss_mean': np.float64(0.7077530383498942),
 'training/train_loss_std': np.float64(0.4243110473270608)}
Step 0 - mean train loss  0.667707
Step 100 - mean train loss  0.797574
Step 200 - mean train loss  0.860692
Step 300 - mean train loss  0.823175
Step 400 - mean train loss  0.822455
Step 500 - mean train loss  0.816486
Step 600 - mean train loss  0.779519
Step 700 - mean train loss  0.778458
Step 800 - mean train loss  0.767872
Step 900 - mean train loss  0.748588
Epoch 16 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #16 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.98904585838318,
 'training/train_loss_mean': np.float64(0.7314260314409453),
 'training/train_loss_std': np.float64(0.3932274302338005)}
Best model saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/best_model
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.507219589760244),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.507219589760244),
 'mean_qoe': np.float64(0.0009589828914383498),
 'time/evaluation': 235.66938948631287,
 'total_qoe': np.float64(4.507219589760244)}
Step 0 - mean train loss  0.966531
Step 100 - mean train loss  0.599899
Step 200 - mean train loss  0.619261
Step 300 - mean train loss  0.589447
Step 400 - mean train loss  0.568922
Step 500 - mean train loss  0.551000
Step 600 - mean train loss  0.553374
Step 700 - mean train loss  0.557337
Step 800 - mean train loss  0.554093
Step 900 - mean train loss  0.558997
Epoch 17 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #17 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.13962030410767,
 'training/train_loss_mean': np.float64(0.584125119414794),
 'training/train_loss_std': np.float64(0.3552296026133205)}
Step 0 - mean train loss  0.698630
Step 100 - mean train loss  0.561614
Step 200 - mean train loss  0.574762
Step 300 - mean train loss  0.574268
Step 400 - mean train loss  0.569232
Step 500 - mean train loss  0.557239
Step 600 - mean train loss  0.558737
Step 700 - mean train loss  0.556665
Step 800 - mean train loss  0.556254
Step 900 - mean train loss  0.557998
Epoch 18 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #18 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.35567164421082,
 'training/train_loss_mean': np.float64(0.5707369761206733),
 'training/train_loss_std': np.float64(0.35031356742377223)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.507219589760244),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.504123636257192),
 'mean_qoe': np.float64(0.0009583241779270621),
 'time/evaluation': 234.87544775009155,
 'total_qoe': np.float64(4.504123636257192)}
Step 0 - mean train loss  0.263439
Step 100 - mean train loss  0.474956
Step 200 - mean train loss  0.489063
Step 300 - mean train loss  0.514617
Step 400 - mean train loss  0.536364
Step 500 - mean train loss  0.533113
Step 600 - mean train loss  0.562911
Step 700 - mean train loss  0.574019
Step 800 - mean train loss  0.570873
Step 900 - mean train loss  0.565383
Epoch 19 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #19 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.92996835708618,
 'training/train_loss_mean': np.float64(0.5744159867244597),
 'training/train_loss_std': np.float64(0.3488501096226186)}
Step 0 - mean train loss  0.714182
Step 100 - mean train loss  0.562916
Step 200 - mean train loss  0.548110
Step 300 - mean train loss  0.520756
Step 400 - mean train loss  0.545373
Step 500 - mean train loss  0.539215
Step 600 - mean train loss  0.530691
Step 700 - mean train loss  0.528697
Step 800 - mean train loss  0.517401
Step 900 - mean train loss  0.522760
Epoch 20 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #20 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.38199949264526,
 'training/train_loss_mean': np.float64(0.5272526018775955),
 'training/train_loss_std': np.float64(0.31075213953407543)}
Best model saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/best_model
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.510929003039911),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.510929003039911),
 'mean_qoe': np.float64(0.0009597721283063641),
 'time/evaluation': 233.5802412033081,
 'total_qoe': np.float64(4.510929003039911)}
Step 0 - mean train loss  0.435704
Step 100 - mean train loss  0.506361
Step 200 - mean train loss  0.491230
Step 300 - mean train loss  0.484378
Step 400 - mean train loss  0.470548
Step 500 - mean train loss  0.471817
Step 600 - mean train loss  0.483779
Step 700 - mean train loss  0.478377
Step 800 - mean train loss  0.480262
Step 900 - mean train loss  0.479888
Epoch 21 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #21 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.35765337944031,
 'training/train_loss_mean': np.float64(0.5098165594168516),
 'training/train_loss_std': np.float64(0.3269639162735272)}
Step 0 - mean train loss  0.535266
Step 100 - mean train loss  0.552018
Step 200 - mean train loss  0.516578
Step 300 - mean train loss  0.509193
Step 400 - mean train loss  0.489722
Step 500 - mean train loss  0.479777
Step 600 - mean train loss  0.480778
Step 700 - mean train loss  0.485688
Step 800 - mean train loss  0.482804
Step 900 - mean train loss  0.483872
Epoch 22 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #22 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.30230784416199,
 'training/train_loss_mean': np.float64(0.48479902541618347),
 'training/train_loss_std': np.float64(0.26912738398886443)}
Best model saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/best_model
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.520624922729373),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.520624922729373),
 'mean_qoe': np.float64(0.0009618350899424197),
 'time/evaluation': 233.1891269683838,
 'total_qoe': np.float64(4.520624922729373)}
Step 0 - mean train loss  0.509411
Step 100 - mean train loss  0.669077
Step 200 - mean train loss  0.867336
Step 300 - mean train loss  0.760595
Step 400 - mean train loss  0.705905
Step 500 - mean train loss  0.662686
Step 600 - mean train loss  0.647382
Step 700 - mean train loss  0.671764
Step 800 - mean train loss  0.658614
Step 900 - mean train loss  0.659949
Epoch 23 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #23 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.45335364341736,
 'training/train_loss_mean': np.float64(0.6456667345711946),
 'training/train_loss_std': np.float64(0.42906755771287486)}
Step 0 - mean train loss  0.790384
Step 100 - mean train loss  0.625177
Step 200 - mean train loss  0.564634
Step 300 - mean train loss  0.539736
Step 400 - mean train loss  0.543012
Step 500 - mean train loss  0.529662
Step 600 - mean train loss  0.522294
Step 700 - mean train loss  0.522446
Step 800 - mean train loss  0.516500
Step 900 - mean train loss  0.513479
Epoch 24 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #24 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.76087141036987,
 'training/train_loss_mean': np.float64(0.5144239074146622),
 'training/train_loss_std': np.float64(0.3159198556230701)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.520624922729373),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.51368021725364),
 'mean_qoe': np.float64(0.0009603574930326893),
 'time/evaluation': 233.00616455078125,
 'total_qoe': np.float64(4.51368021725364)}
Step 0 - mean train loss  0.234886
Step 100 - mean train loss  0.412273
Step 200 - mean train loss  0.438838
Step 300 - mean train loss  0.449191
Step 400 - mean train loss  0.461410
Step 500 - mean train loss  0.468283
Step 600 - mean train loss  0.498757
Step 700 - mean train loss  0.508259
Step 800 - mean train loss  0.514621
Step 900 - mean train loss  0.521598
Epoch 25 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #25 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.83523416519165,
 'training/train_loss_mean': np.float64(0.5507291319343377),
 'training/train_loss_std': np.float64(0.34964217712994367)}
Step 0 - mean train loss  0.657028
Step 100 - mean train loss  0.530523
Step 200 - mean train loss  0.506976
Step 300 - mean train loss  0.537319
Step 400 - mean train loss  0.533682
Step 500 - mean train loss  0.517532
Step 600 - mean train loss  0.511479
Step 700 - mean train loss  0.513103
Step 800 - mean train loss  0.512014
Step 900 - mean train loss  0.510539
Epoch 26 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #26 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.476482629776,
 'training/train_loss_mean': np.float64(0.5063617665186921),
 'training/train_loss_std': np.float64(0.29034177013804335)}
Best model saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/best_model
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.520803991657757),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.520803991657757),
 'mean_qoe': np.float64(0.0009618731897144163),
 'time/evaluation': 233.28251552581787,
 'total_qoe': np.float64(4.520803991657757)}
Step 0 - mean train loss  0.296264
Step 100 - mean train loss  0.471095
Step 200 - mean train loss  0.484592
Step 300 - mean train loss  0.492018
Step 400 - mean train loss  0.485304
Step 500 - mean train loss  0.483257
Step 600 - mean train loss  0.527212
Step 700 - mean train loss  0.538262
Step 800 - mean train loss  0.533548
Step 900 - mean train loss  0.525968
Epoch 27 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #27 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.96633291244507,
 'training/train_loss_mean': np.float64(0.522255045560284),
 'training/train_loss_std': np.float64(0.33268677986687595)}
Step 0 - mean train loss  0.566734
Step 100 - mean train loss  0.455577
Step 200 - mean train loss  0.448589
Step 300 - mean train loss  0.459296
Step 400 - mean train loss  0.461728
Step 500 - mean train loss  0.476293
Step 600 - mean train loss  0.484279
Step 700 - mean train loss  0.494527
Step 800 - mean train loss  0.511240
Step 900 - mean train loss  0.512447
Epoch 28 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #28 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.61674690246582,
 'training/train_loss_mean': np.float64(0.5127615833509598),
 'training/train_loss_std': np.float64(0.32131273056332293)}
Best model saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/best_model
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.520932871260249),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.520932871260249),
 'mean_qoe': np.float64(0.0009619006109064359),
 'time/evaluation': 232.9400978088379,
 'total_qoe': np.float64(4.520932871260249)}
Step 0 - mean train loss  0.604332
Step 100 - mean train loss  0.554460
Step 200 - mean train loss  0.584014
Step 300 - mean train loss  0.541925
Step 400 - mean train loss  0.532131
Step 500 - mean train loss  0.541830
Step 600 - mean train loss  0.601179
Step 700 - mean train loss  0.647163
Step 800 - mean train loss  0.638213
Step 900 - mean train loss  0.637853
Epoch 29 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #29 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.31081247329712,
 'training/train_loss_mean': np.float64(0.6272664420001852),
 'training/train_loss_std': np.float64(0.4351362116737072)}
Step 0 - mean train loss  0.803485
Step 100 - mean train loss  0.708193
Step 200 - mean train loss  0.613907
Step 300 - mean train loss  0.582695
Step 400 - mean train loss  0.567482
Step 500 - mean train loss  0.556450
Step 600 - mean train loss  0.543653
Step 700 - mean train loss  0.552490
Step 800 - mean train loss  0.556796
Step 900 - mean train loss  0.557015
Epoch 30 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #30 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.62737154960632,
 'training/train_loss_mean': np.float64(0.5574528440710599),
 'training/train_loss_std': np.float64(0.3518001726601217)}
Best model saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/best_model
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.525194102161407),
 'mean_qoe': np.float64(0.0009628072557790228),
 'time/evaluation': 234.25405406951904,
 'total_qoe': np.float64(4.525194102161407)}
Step 0 - mean train loss  0.536911
Step 100 - mean train loss  0.552577
Step 200 - mean train loss  0.499375
Step 300 - mean train loss  0.473459
Step 400 - mean train loss  0.458010
Step 500 - mean train loss  0.450006
Step 600 - mean train loss  0.458620
Step 700 - mean train loss  0.492103
Step 800 - mean train loss  0.500738
Step 900 - mean train loss  0.513848
Epoch 31 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #31 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.2970974445343,
 'training/train_loss_mean': np.float64(0.5195480243255661),
 'training/train_loss_std': np.float64(0.33886977503197796)}
Step 0 - mean train loss  0.727868
Step 100 - mean train loss  0.603346
Step 200 - mean train loss  0.550278
Step 300 - mean train loss  0.535514
Step 400 - mean train loss  0.518935
Step 500 - mean train loss  0.510298
Step 600 - mean train loss  0.500385
Step 700 - mean train loss  0.505166
Step 800 - mean train loss  0.505527
Step 900 - mean train loss  0.502778
Epoch 32 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #32 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.23043584823608,
 'training/train_loss_mean': np.float64(0.5032002633260086),
 'training/train_loss_std': np.float64(0.3004580512438287)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.519278892178752),
 'mean_qoe': np.float64(0.0009615487004635643),
 'time/evaluation': 234.19749855995178,
 'total_qoe': np.float64(4.519278892178752)}
Step 0 - mean train loss  0.539542
Step 100 - mean train loss  0.521651
Step 200 - mean train loss  0.483364
Step 300 - mean train loss  0.471248
Step 400 - mean train loss  0.466787
Step 500 - mean train loss  0.454851
Step 600 - mean train loss  0.466804
Step 700 - mean train loss  0.477460
Step 800 - mean train loss  0.481879
Step 900 - mean train loss  0.479140
Epoch 33 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #33 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.94486594200134,
 'training/train_loss_mean': np.float64(0.5004346899821023),
 'training/train_loss_std': np.float64(0.3255306595290285)}
Step 0 - mean train loss  0.684828
Step 100 - mean train loss  0.618784
Step 200 - mean train loss  0.612510
Step 300 - mean train loss  0.641063
Step 400 - mean train loss  0.616798
Step 500 - mean train loss  0.588807
Step 600 - mean train loss  0.573160
Step 700 - mean train loss  0.560102
Step 800 - mean train loss  0.548258
Step 900 - mean train loss  0.537758
Epoch 34 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #34 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.81158804893494,
 'training/train_loss_mean': np.float64(0.5437028558330328),
 'training/train_loss_std': np.float64(0.3959411951534932)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.519094622203017),
 'mean_qoe': np.float64(0.0009615094940857483),
 'time/evaluation': 233.94957208633423,
 'total_qoe': np.float64(4.519094622203017)}
Step 0 - mean train loss  0.334269
Step 100 - mean train loss  0.632051
Step 200 - mean train loss  0.598601
Step 300 - mean train loss  0.554226
Step 400 - mean train loss  0.519114
Step 500 - mean train loss  0.493011
Step 600 - mean train loss  0.497936
Step 700 - mean train loss  0.505213
Step 800 - mean train loss  0.506095
Step 900 - mean train loss  0.497124
Epoch 35 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #35 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.92597460746765,
 'training/train_loss_mean': np.float64(0.5091649381599078),
 'training/train_loss_std': np.float64(0.3240364216567819)}
Step 0 - mean train loss  0.596300
Step 100 - mean train loss  0.580897
Step 200 - mean train loss  0.562437
Step 300 - mean train loss  0.568771
Step 400 - mean train loss  0.561448
Step 500 - mean train loss  0.563373
Step 600 - mean train loss  0.554882
Step 700 - mean train loss  0.563079
Step 800 - mean train loss  0.574266
Step 900 - mean train loss  0.571729
Epoch 36 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #36 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.03638982772827,
 'training/train_loss_mean': np.float64(0.5721229084810765),
 'training/train_loss_std': np.float64(0.36387464640460404)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.517084839033149),
 'mean_qoe': np.float64(0.0009610818806453509),
 'time/evaluation': 234.40653252601624,
 'total_qoe': np.float64(4.517084839033149)}
Step 0 - mean train loss  0.487222
Step 100 - mean train loss  0.507933
Step 200 - mean train loss  0.486164
Step 300 - mean train loss  0.480421
Step 400 - mean train loss  0.483145
Step 500 - mean train loss  0.480002
Step 600 - mean train loss  0.485983
Step 700 - mean train loss  0.486507
Step 800 - mean train loss  0.487339
Step 900 - mean train loss  0.482730
Epoch 37 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #37 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.8791139125824,
 'training/train_loss_mean': np.float64(0.48696167290352854),
 'training/train_loss_std': np.float64(0.31391356104389717)}
Step 0 - mean train loss  0.614007
Step 100 - mean train loss  0.568173
Step 200 - mean train loss  0.524915
Step 300 - mean train loss  0.513882
Step 400 - mean train loss  0.498974
Step 500 - mean train loss  0.494282
Step 600 - mean train loss  0.487916
Step 700 - mean train loss  0.490062
Step 800 - mean train loss  0.486839
Step 900 - mean train loss  0.483748
Epoch 38 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #38 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.1609275341034,
 'training/train_loss_mean': np.float64(0.4904269362355902),
 'training/train_loss_std': np.float64(0.2989302254155403)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5161145826749225),
 'mean_qoe': np.float64(0.0009608754431223239),
 'time/evaluation': 233.79205775260925,
 'total_qoe': np.float64(4.5161145826749225)}
Step 0 - mean train loss  0.277674
Step 100 - mean train loss  0.450355
Step 200 - mean train loss  0.461830
Step 300 - mean train loss  0.470919
Step 400 - mean train loss  0.461702
Step 500 - mean train loss  0.450329
Step 600 - mean train loss  0.451848
Step 700 - mean train loss  0.457012
Step 800 - mean train loss  0.468488
Step 900 - mean train loss  0.465960
Epoch 39 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #39 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.55528330802917,
 'training/train_loss_mean': np.float64(0.47774986975291467),
 'training/train_loss_std': np.float64(0.30723457718850106)}
Checkpoint saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/39
Step 0 - mean train loss  0.727392
Step 100 - mean train loss  0.684930
Step 200 - mean train loss  0.627604
Step 300 - mean train loss  0.617054
Step 400 - mean train loss  0.582626
Step 500 - mean train loss  0.568387
Step 600 - mean train loss  0.548310
Step 700 - mean train loss  0.554379
Step 800 - mean train loss  0.550168
Step 900 - mean train loss  0.539247
Epoch 40 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #40 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.77386212348938,
 'training/train_loss_mean': np.float64(0.5363706470388605),
 'training/train_loss_std': np.float64(0.3521539961522068)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.518361684549035),
 'mean_qoe': np.float64(0.00096135354990405),
 'time/evaluation': 235.64903664588928,
 'total_qoe': np.float64(4.518361684549035)}
Step 0 - mean train loss  0.250117
Step 100 - mean train loss  0.451173
Step 200 - mean train loss  0.495036
Step 300 - mean train loss  0.514440
Step 400 - mean train loss  0.516677
Step 500 - mean train loss  0.496866
Step 600 - mean train loss  0.502472
Step 700 - mean train loss  0.500077
Step 800 - mean train loss  0.499338
Step 900 - mean train loss  0.493484
Epoch 41 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #41 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.66199588775635,
 'training/train_loss_mean': np.float64(0.4948128895608738),
 'training/train_loss_std': np.float64(0.3199913193381247)}
Step 0 - mean train loss  0.559222
Step 100 - mean train loss  0.660120
Step 200 - mean train loss  0.611775
Step 300 - mean train loss  0.592073
Step 400 - mean train loss  0.568358
Step 500 - mean train loss  0.589420
Step 600 - mean train loss  0.581211
Step 700 - mean train loss  0.585920
Step 800 - mean train loss  0.599278
Step 900 - mean train loss  0.592434
Epoch 42 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #42 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.09795022010803,
 'training/train_loss_mean': np.float64(0.595348648077013),
 'training/train_loss_std': np.float64(0.3919342011710652)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5228008380869476),
 'mean_qoe': np.float64(0.0009622980506567973),
 'time/evaluation': 235.77735662460327,
 'total_qoe': np.float64(4.5228008380869476)}
Step 0 - mean train loss  0.365406
Step 100 - mean train loss  0.528150
Step 200 - mean train loss  0.520338
Step 300 - mean train loss  0.506245
Step 400 - mean train loss  0.508219
Step 500 - mean train loss  0.497106
Step 600 - mean train loss  0.504035
Step 700 - mean train loss  0.509850
Step 800 - mean train loss  0.507664
Step 900 - mean train loss  0.502902
Epoch 43 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #43 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.03539514541626,
 'training/train_loss_mean': np.float64(0.5061903573552999),
 'training/train_loss_std': np.float64(0.32159098769608546)}
Step 0 - mean train loss  0.611364
Step 100 - mean train loss  0.611906
Step 200 - mean train loss  0.544064
Step 300 - mean train loss  0.544746
Step 400 - mean train loss  0.524670
Step 500 - mean train loss  0.559463
Step 600 - mean train loss  0.563418
Step 700 - mean train loss  0.571699
Step 800 - mean train loss  0.590653
Step 900 - mean train loss  0.589601
Epoch 44 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #44 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.23823428153992,
 'training/train_loss_mean': np.float64(0.6054127395167539),
 'training/train_loss_std': np.float64(0.4085582049128573)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.509163062601788),
 'mean_qoe': np.float64(0.0009593963962982527),
 'time/evaluation': 235.08323788642883,
 'total_qoe': np.float64(4.509163062601788)}
Step 0 - mean train loss  0.171794
Step 100 - mean train loss  0.531307
Step 200 - mean train loss  0.662596
Step 300 - mean train loss  0.667375
Step 400 - mean train loss  0.744263
Step 500 - mean train loss  0.801686
Step 600 - mean train loss  0.795561
Step 700 - mean train loss  0.854138
Step 800 - mean train loss  0.826266
Step 900 - mean train loss  0.799584
Epoch 45 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #45 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.1599977016449,
 'training/train_loss_mean': np.float64(0.7808350963169947),
 'training/train_loss_std': np.float64(0.6121960633584351)}
Step 0 - mean train loss  0.585102
Step 100 - mean train loss  0.625645
Step 200 - mean train loss  0.560169
Step 300 - mean train loss  0.559736
Step 400 - mean train loss  0.547192
Step 500 - mean train loss  0.592174
Step 600 - mean train loss  0.577508
Step 700 - mean train loss  0.575809
Step 800 - mean train loss  0.564746
Step 900 - mean train loss  0.569421
Epoch 46 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #46 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.1613655090332,
 'training/train_loss_mean': np.float64(0.5612256079449329),
 'training/train_loss_std': np.float64(0.3649063292389013)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513850162390618),
 'mean_qoe': np.float64(0.0009603936515724719),
 'time/evaluation': 233.57949447631836,
 'total_qoe': np.float64(4.513850162390618)}
Step 0 - mean train loss  0.258402
Step 100 - mean train loss  0.448741
Step 200 - mean train loss  0.472329
Step 300 - mean train loss  0.473489
Step 400 - mean train loss  0.471815
Step 500 - mean train loss  0.467779
Step 600 - mean train loss  0.478986
Step 700 - mean train loss  0.480466
Step 800 - mean train loss  0.483285
Step 900 - mean train loss  0.475921
Epoch 47 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #47 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.49239349365234,
 'training/train_loss_mean': np.float64(0.4837504567187579),
 'training/train_loss_std': np.float64(0.31743065883568244)}
Step 0 - mean train loss  0.759364
Step 100 - mean train loss  0.683140
Step 200 - mean train loss  0.639689
Step 300 - mean train loss  0.632915
Step 400 - mean train loss  0.599772
Step 500 - mean train loss  0.623371
Step 600 - mean train loss  0.612609
Step 700 - mean train loss  0.594235
Step 800 - mean train loss  0.582736
Step 900 - mean train loss  0.576272
Epoch 48 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #48 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.84970426559448,
 'training/train_loss_mean': np.float64(0.5794633690099023),
 'training/train_loss_std': np.float64(0.37086863149522553)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.515613654831961),
 'mean_qoe': np.float64(0.0009607688627302045),
 'time/evaluation': 235.75156164169312,
 'total_qoe': np.float64(4.515613654831961)}
Step 0 - mean train loss  0.290283
Step 100 - mean train loss  0.508251
Step 200 - mean train loss  0.668097
Step 300 - mean train loss  0.673785
Step 400 - mean train loss  0.715296
Step 500 - mean train loss  0.779600
Step 600 - mean train loss  0.782076
Step 700 - mean train loss  0.903761
Step 800 - mean train loss  0.943715
Step 900 - mean train loss  0.912423
Epoch 49 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #49 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.5714602470398,
 'training/train_loss_mean': np.float64(0.9165732474407615),
 'training/train_loss_std': np.float64(0.7699141028117312)}
Step 0 - mean train loss  1.445446
Step 100 - mean train loss  0.680875
Step 200 - mean train loss  0.650456
Step 300 - mean train loss  0.633185
Step 400 - mean train loss  0.610982
Step 500 - mean train loss  0.602299
Step 600 - mean train loss  0.596065
Step 700 - mean train loss  0.584022
Step 800 - mean train loss  0.571048
Step 900 - mean train loss  0.561297
Epoch 50 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #50 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.19761753082275,
 'training/train_loss_mean': np.float64(0.5533055645941072),
 'training/train_loss_std': np.float64(0.3746128079573139)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5176242044355215),
 'mean_qoe': np.float64(0.0009611966392416003),
 'time/evaluation': 234.86542987823486,
 'total_qoe': np.float64(4.5176242044355215)}
Step 0 - mean train loss  0.427040
Step 100 - mean train loss  0.514184
Step 200 - mean train loss  0.507378
Step 300 - mean train loss  0.503227
Step 400 - mean train loss  0.496291
Step 500 - mean train loss  0.501796
Step 600 - mean train loss  0.519133
Step 700 - mean train loss  0.533346
Step 800 - mean train loss  0.530990
Step 900 - mean train loss  0.527448
Epoch 51 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #51 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.07405996322632,
 'training/train_loss_mean': np.float64(0.5286178244503419),
 'training/train_loss_std': np.float64(0.35448005000150223)}
Step 0 - mean train loss  0.908701
Step 100 - mean train loss  0.602374
Step 200 - mean train loss  0.550132
Step 300 - mean train loss  0.545627
Step 400 - mean train loss  0.518836
Step 500 - mean train loss  0.514875
Step 600 - mean train loss  0.508062
Step 700 - mean train loss  0.517243
Step 800 - mean train loss  0.512565
Step 900 - mean train loss  0.506039
Epoch 52 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #52 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.11217260360718,
 'training/train_loss_mean': np.float64(0.5048695354276883),
 'training/train_loss_std': np.float64(0.3470242451590153)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.515794780482748),
 'mean_qoe': np.float64(0.0009608074001027124),
 'time/evaluation': 234.7640883922577,
 'total_qoe': np.float64(4.515794780482748)}
Step 0 - mean train loss  0.447813
Step 100 - mean train loss  0.473428
Step 200 - mean train loss  0.523638
Step 300 - mean train loss  0.523705
Step 400 - mean train loss  0.519362
Step 500 - mean train loss  0.502711
Step 600 - mean train loss  0.507659
Step 700 - mean train loss  0.521985
Step 800 - mean train loss  0.525098
Step 900 - mean train loss  0.523984
Epoch 53 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #53 ====================
>>>>>>>>>> Training Information:
{'time/training': 112.97498869895935,
 'training/train_loss_mean': np.float64(0.5304618097465844),
 'training/train_loss_std': np.float64(0.3747745428889042)}
Step 0 - mean train loss  1.031558
Step 100 - mean train loss  0.539548
Step 200 - mean train loss  0.501672
Step 300 - mean train loss  0.501988
Step 400 - mean train loss  0.478807
Step 500 - mean train loss  0.485618
Step 600 - mean train loss  0.492483
Step 700 - mean train loss  0.495656
Step 800 - mean train loss  0.497228
Step 900 - mean train loss  0.503514
Epoch 54 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #54 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.0406801700592,
 'training/train_loss_mean': np.float64(0.5086556497889283),
 'training/train_loss_std': np.float64(0.3487321563494526)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.511573684044397),
 'mean_qoe': np.float64(0.0009599092944775313),
 'time/evaluation': 232.95450067520142,
 'total_qoe': np.float64(4.511573684044397)}
Step 0 - mean train loss  0.419841
Step 100 - mean train loss  0.497145
Step 200 - mean train loss  0.512520
Step 300 - mean train loss  0.510409
Step 400 - mean train loss  0.510993
Step 500 - mean train loss  0.502638
Step 600 - mean train loss  0.503878
Step 700 - mean train loss  0.516891
Step 800 - mean train loss  0.521034
Step 900 - mean train loss  0.513578
Epoch 55 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #55 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.56733441352844,
 'training/train_loss_mean': np.float64(0.5188211797884793),
 'training/train_loss_std': np.float64(0.35939205217574444)}
Step 0 - mean train loss  0.772234
Step 100 - mean train loss  0.613334
Step 200 - mean train loss  0.567444
Step 300 - mean train loss  0.586078
Step 400 - mean train loss  0.554862
Step 500 - mean train loss  0.555831
Step 600 - mean train loss  0.551985
Step 700 - mean train loss  0.553264
Step 800 - mean train loss  0.550328
Step 900 - mean train loss  0.542825
Epoch 56 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #56 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.57306337356567,
 'training/train_loss_mean': np.float64(0.541564479040241),
 'training/train_loss_std': np.float64(0.37390489463560594)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.517306055661874),
 'mean_qoe': np.float64(0.0009611289480131646),
 'time/evaluation': 234.7350058555603,
 'total_qoe': np.float64(4.517306055661874)}
Step 0 - mean train loss  0.493700
Step 100 - mean train loss  0.514897
Step 200 - mean train loss  0.520057
Step 300 - mean train loss  0.519148
Step 400 - mean train loss  0.515310
Step 500 - mean train loss  0.502223
Step 600 - mean train loss  0.504847
Step 700 - mean train loss  0.519326
Step 800 - mean train loss  0.525526
Step 900 - mean train loss  0.534369
Epoch 57 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #57 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.52303147315979,
 'training/train_loss_mean': np.float64(0.5443810422607439),
 'training/train_loss_std': np.float64(0.39652035339697583)}
Step 0 - mean train loss  1.036828
Step 100 - mean train loss  0.576246
Step 200 - mean train loss  0.536920
Step 300 - mean train loss  0.539511
Step 400 - mean train loss  0.520642
Step 500 - mean train loss  0.545056
Step 600 - mean train loss  0.550109
Step 700 - mean train loss  0.548801
Step 800 - mean train loss  0.551154
Step 900 - mean train loss  0.551085
Epoch 58 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #58 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.53454875946045,
 'training/train_loss_mean': np.float64(0.5564757275081148),
 'training/train_loss_std': np.float64(0.3930593884376142)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.516403197476139),
 'mean_qoe': np.float64(0.0009609368505268381),
 'time/evaluation': 234.73070120811462,
 'total_qoe': np.float64(4.516403197476139)}
Step 0 - mean train loss  0.417613
Step 100 - mean train loss  0.583195
Step 200 - mean train loss  0.582981
Step 300 - mean train loss  0.584444
Step 400 - mean train loss  0.579379
Step 500 - mean train loss  0.579181
Step 600 - mean train loss  0.577121
Step 700 - mean train loss  0.586835
Step 800 - mean train loss  0.592251
Step 900 - mean train loss  0.584046
Epoch 59 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #59 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.58063864707947,
 'training/train_loss_mean': np.float64(0.5821224206860549),
 'training/train_loss_std': np.float64(0.4090117302483933)}
Step 0 - mean train loss  1.008155
Step 100 - mean train loss  0.673916
Step 200 - mean train loss  0.694259
Step 300 - mean train loss  0.719292
Step 400 - mean train loss  0.717971
Step 500 - mean train loss  0.697950
Step 600 - mean train loss  0.685874
Step 700 - mean train loss  0.681252
Step 800 - mean train loss  0.673053
Step 900 - mean train loss  0.663832
Epoch 60 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #60 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.56662678718567,
 'training/train_loss_mean': np.float64(0.6597643219580664),
 'training/train_loss_std': np.float64(0.6653163060918887)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5128802837524065),
 'mean_qoe': np.float64(0.0009601872944154056),
 'time/evaluation': 234.10543251037598,
 'total_qoe': np.float64(4.5128802837524065)}
Step 0 - mean train loss  0.417830
Step 100 - mean train loss  0.559392
Step 200 - mean train loss  0.580812
Step 300 - mean train loss  0.587193
Step 400 - mean train loss  0.561720
Step 500 - mean train loss  0.546634
Step 600 - mean train loss  0.556959
Step 700 - mean train loss  0.568087
Step 800 - mean train loss  0.569417
Step 900 - mean train loss  0.564026
Epoch 61 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #61 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.84801816940308,
 'training/train_loss_mean': np.float64(0.5663761938664965),
 'training/train_loss_std': np.float64(0.42138917835682443)}
Step 0 - mean train loss  1.009413
Step 100 - mean train loss  0.656572
Step 200 - mean train loss  0.591004
Step 300 - mean train loss  0.591919
Step 400 - mean train loss  0.561510
Step 500 - mean train loss  0.559141
Step 600 - mean train loss  0.562569
Step 700 - mean train loss  0.560839
Step 800 - mean train loss  0.559030
Step 900 - mean train loss  0.563551
Epoch 62 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #62 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.51714277267456,
 'training/train_loss_mean': np.float64(0.5761120589646365),
 'training/train_loss_std': np.float64(0.4229414160900254)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5207274825798605),
 'mean_qoe': np.float64(0.0009618569111872043),
 'time/evaluation': 235.4852557182312,
 'total_qoe': np.float64(4.5207274825798605)}
Step 0 - mean train loss  0.372828
Step 100 - mean train loss  0.662181
Step 200 - mean train loss  0.682697
Step 300 - mean train loss  0.719639
Step 400 - mean train loss  0.733008
Step 500 - mean train loss  0.698563
Step 600 - mean train loss  0.688832
Step 700 - mean train loss  0.676806
Step 800 - mean train loss  0.670773
Step 900 - mean train loss  0.659015
Epoch 63 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #63 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.00553250312805,
 'training/train_loss_mean': np.float64(0.6564763335937244),
 'training/train_loss_std': np.float64(0.49346997046580654)}
Step 0 - mean train loss  1.102922
Step 100 - mean train loss  0.767863
Step 200 - mean train loss  0.668287
Step 300 - mean train loss  0.692395
Step 400 - mean train loss  0.663567
Step 500 - mean train loss  0.704923
Step 600 - mean train loss  0.698165
Step 700 - mean train loss  0.687146
Step 800 - mean train loss  0.673248
Step 900 - mean train loss  0.688536
Epoch 64 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #64 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.04068541526794,
 'training/train_loss_mean': np.float64(0.6900503815439984),
 'training/train_loss_std': np.float64(0.5193397822581719)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.516686082374194),
 'mean_qoe': np.float64(0.0009609970388030199),
 'time/evaluation': 235.29568886756897,
 'total_qoe': np.float64(4.516686082374194)}
Step 0 - mean train loss  0.297137
Step 100 - mean train loss  0.745200
Step 200 - mean train loss  0.826840
Step 300 - mean train loss  0.923991
Step 400 - mean train loss  0.860481
Step 500 - mean train loss  0.822889
Step 600 - mean train loss  0.799902
Step 700 - mean train loss  0.796703
Step 800 - mean train loss  0.803769
Step 900 - mean train loss  0.776618
Epoch 65 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #65 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.89507722854614,
 'training/train_loss_mean': np.float64(0.7715683286017495),
 'training/train_loss_std': np.float64(0.5937044488098048)}
Step 0 - mean train loss  1.493935
Step 100 - mean train loss  0.672421
Step 200 - mean train loss  0.623037
Step 300 - mean train loss  0.637869
Step 400 - mean train loss  0.609281
Step 500 - mean train loss  0.622535
Step 600 - mean train loss  0.621152
Step 700 - mean train loss  0.613387
Step 800 - mean train loss  0.603705
Step 900 - mean train loss  0.616381
Epoch 66 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #66 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.83931827545166,
 'training/train_loss_mean': np.float64(0.6173324077921951),
 'training/train_loss_std': np.float64(0.45317941848178295)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513386597801097),
 'mean_qoe': np.float64(0.0009602950208087441),
 'time/evaluation': 235.45857310295105,
 'total_qoe': np.float64(4.513386597801097)}
Step 0 - mean train loss  0.318909
Step 100 - mean train loss  0.571185
Step 200 - mean train loss  0.617616
Step 300 - mean train loss  0.619909
Step 400 - mean train loss  0.610302
Step 500 - mean train loss  0.592662
Step 600 - mean train loss  0.591101
Step 700 - mean train loss  0.589756
Step 800 - mean train loss  0.586607
Step 900 - mean train loss  0.579718
Epoch 67 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #67 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.6055281162262,
 'training/train_loss_mean': np.float64(0.5906060004716012),
 'training/train_loss_std': np.float64(0.4637711147907274)}
Step 0 - mean train loss  1.311523
Step 100 - mean train loss  0.647532
Step 200 - mean train loss  0.592704
Step 300 - mean train loss  0.613896
Step 400 - mean train loss  0.585159
Step 500 - mean train loss  0.584387
Step 600 - mean train loss  0.588230
Step 700 - mean train loss  0.594174
Step 800 - mean train loss  0.595324
Step 900 - mean train loss  0.600064
Epoch 68 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #68 ====================
>>>>>>>>>> Training Information:
{'time/training': 112.89485621452332,
 'training/train_loss_mean': np.float64(0.6021533750556783),
 'training/train_loss_std': np.float64(0.44008900721070526)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.50764662163354),
 'mean_qoe': np.float64(0.0009590737492837318),
 'time/evaluation': 235.36610436439514,
 'total_qoe': np.float64(4.50764662163354)}
Step 0 - mean train loss  0.368936
Step 100 - mean train loss  0.570461
Step 200 - mean train loss  0.620665
Step 300 - mean train loss  0.628770
Step 400 - mean train loss  0.619362
Step 500 - mean train loss  0.596008
Step 600 - mean train loss  0.594666
Step 700 - mean train loss  0.596489
Step 800 - mean train loss  0.595944
Step 900 - mean train loss  0.585645
Epoch 69 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #69 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.89594388008118,
 'training/train_loss_mean': np.float64(0.586541525498042),
 'training/train_loss_std': np.float64(0.4569906603533564)}
Step 0 - mean train loss  1.142714
Step 100 - mean train loss  0.667344
Step 200 - mean train loss  0.620765
Step 300 - mean train loss  0.627399
Step 400 - mean train loss  0.594450
Step 500 - mean train loss  0.587004
Step 600 - mean train loss  0.596165
Step 700 - mean train loss  0.594869
Step 800 - mean train loss  0.589626
Step 900 - mean train loss  0.592871
Epoch 70 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #70 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.42981934547424,
 'training/train_loss_mean': np.float64(0.6133302296497803),
 'training/train_loss_std': np.float64(0.506051053530072)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.505075134106579),
 'mean_qoe': np.float64(0.0009585266242779955),
 'time/evaluation': 235.88215947151184,
 'total_qoe': np.float64(4.505075134106579)}
Step 0 - mean train loss  0.373040
Step 100 - mean train loss  0.611162
Step 200 - mean train loss  0.679961
Step 300 - mean train loss  0.706954
Step 400 - mean train loss  0.704613
Step 500 - mean train loss  0.729931
Step 600 - mean train loss  0.724837
Step 700 - mean train loss  0.758942
Step 800 - mean train loss  0.756199
Step 900 - mean train loss  0.752684
Epoch 71 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #71 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.2506492137909,
 'training/train_loss_mean': np.float64(0.7442782549992863),
 'training/train_loss_std': np.float64(0.5765285827981803)}
Step 0 - mean train loss  0.900876
Step 100 - mean train loss  0.621676
Step 200 - mean train loss  0.614161
Step 300 - mean train loss  0.622485
Step 400 - mean train loss  0.599284
Step 500 - mean train loss  0.594766
Step 600 - mean train loss  0.601316
Step 700 - mean train loss  0.595401
Step 800 - mean train loss  0.588401
Step 900 - mean train loss  0.586209
Epoch 72 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #72 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.05460357666016,
 'training/train_loss_mean': np.float64(0.5989016685731864),
 'training/train_loss_std': np.float64(0.4802325195930609)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.507004491843614),
 'mean_qoe': np.float64(0.0009589371259241732),
 'time/evaluation': 236.78241777420044,
 'total_qoe': np.float64(4.507004491843614)}
Step 0 - mean train loss  0.365256
Step 100 - mean train loss  0.605840
Step 200 - mean train loss  0.643003
Step 300 - mean train loss  0.708740
Step 400 - mean train loss  0.682302
Step 500 - mean train loss  0.698803
Step 600 - mean train loss  0.734128
Step 700 - mean train loss  0.803541
Step 800 - mean train loss  0.787622
Step 900 - mean train loss  0.770903
Epoch 73 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #73 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.886953830719,
 'training/train_loss_mean': np.float64(0.7603922085578219),
 'training/train_loss_std': np.float64(0.6830830665432007)}
Step 0 - mean train loss  0.957382
Step 100 - mean train loss  0.709549
Step 200 - mean train loss  0.624714
Step 300 - mean train loss  0.628525
Step 400 - mean train loss  0.601619
Step 500 - mean train loss  0.599876
Step 600 - mean train loss  0.600090
Step 700 - mean train loss  0.591138
Step 800 - mean train loss  0.589090
Step 900 - mean train loss  0.595287
Epoch 74 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #74 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.90624260902405,
 'training/train_loss_mean': np.float64(0.5993826596817795),
 'training/train_loss_std': np.float64(0.4626080093095859)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.508489791343755),
 'mean_qoe': np.float64(0.0009592531470944159),
 'time/evaluation': 236.31889605522156,
 'total_qoe': np.float64(4.508489791343755)}
Step 0 - mean train loss  0.511337
Step 100 - mean train loss  0.834015
Step 200 - mean train loss  0.748597
Step 300 - mean train loss  0.865050
Step 400 - mean train loss  0.837582
Step 500 - mean train loss  0.790609
Step 600 - mean train loss  0.782496
Step 700 - mean train loss  0.774922
Step 800 - mean train loss  0.775663
Step 900 - mean train loss  0.750801
Epoch 75 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #75 ====================
>>>>>>>>>> Training Information:
{'time/training': 112.13075351715088,
 'training/train_loss_mean': np.float64(0.7464734749602545),
 'training/train_loss_std': np.float64(0.5925586107605667)}
Step 0 - mean train loss  0.994254
Step 100 - mean train loss  0.777857
Step 200 - mean train loss  0.765420
Step 300 - mean train loss  0.766996
Step 400 - mean train loss  0.737456
Step 500 - mean train loss  0.714742
Step 600 - mean train loss  0.720823
Step 700 - mean train loss  0.705867
Step 800 - mean train loss  0.700346
Step 900 - mean train loss  0.687953
Epoch 76 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #76 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.54357624053955,
 'training/train_loss_mean': np.float64(0.682644298896498),
 'training/train_loss_std': np.float64(0.5354200882478585)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.502037068211641),
 'mean_qoe': np.float64(0.0009578802272790727),
 'time/evaluation': 235.37020182609558,
 'total_qoe': np.float64(4.502037068211641)}
Step 0 - mean train loss  0.457171
Step 100 - mean train loss  0.726903
Step 200 - mean train loss  0.765943
Step 300 - mean train loss  0.729436
Step 400 - mean train loss  0.686748
Step 500 - mean train loss  0.656391
Step 600 - mean train loss  0.661098
Step 700 - mean train loss  0.671226
Step 800 - mean train loss  0.663485
Step 900 - mean train loss  0.646408
Epoch 77 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #77 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.73164772987366,
 'training/train_loss_mean': np.float64(0.644665792357462),
 'training/train_loss_std': np.float64(0.5317401862663921)}
Step 0 - mean train loss  1.078889
Step 100 - mean train loss  0.670957
Step 200 - mean train loss  0.621547
Step 300 - mean train loss  0.650316
Step 400 - mean train loss  0.637137
Step 500 - mean train loss  0.627415
Step 600 - mean train loss  0.631372
Step 700 - mean train loss  0.616334
Step 800 - mean train loss  0.607246
Step 900 - mean train loss  0.600927
Epoch 78 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #78 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.90349912643433,
 'training/train_loss_mean': np.float64(0.6027897927244091),
 'training/train_loss_std': np.float64(0.46904681636649265)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.512588350709394),
 'mean_qoe': np.float64(0.0009601251810019987),
 'time/evaluation': 235.05744576454163,
 'total_qoe': np.float64(4.512588350709394)}
Step 0 - mean train loss  0.442234
Step 100 - mean train loss  0.647701
Step 200 - mean train loss  0.671678
Step 300 - mean train loss  0.651651
Step 400 - mean train loss  0.627385
Step 500 - mean train loss  0.614087
Step 600 - mean train loss  0.621189
Step 700 - mean train loss  0.651687
Step 800 - mean train loss  0.668073
Step 900 - mean train loss  0.671625
Epoch 79 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #79 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.86549139022827,
 'training/train_loss_mean': np.float64(0.679852348138568),
 'training/train_loss_std': np.float64(0.5452854591801422)}
Checkpoint saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/79
Step 0 - mean train loss  1.226206
Step 100 - mean train loss  0.806204
Step 200 - mean train loss  0.708722
Step 300 - mean train loss  0.714587
Step 400 - mean train loss  0.683074
Step 500 - mean train loss  0.670261
Step 600 - mean train loss  0.657797
Step 700 - mean train loss  0.644424
Step 800 - mean train loss  0.634700
Step 900 - mean train loss  0.629940
Epoch 80 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #80 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.83946824073792,
 'training/train_loss_mean': np.float64(0.6322635677549906),
 'training/train_loss_std': np.float64(0.5021325636359504)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.510731933989963),
 'mean_qoe': np.float64(0.0009597301987212688),
 'time/evaluation': 234.77258491516113,
 'total_qoe': np.float64(4.510731933989963)}
Step 0 - mean train loss  0.739889
Step 100 - mean train loss  0.717937
Step 200 - mean train loss  0.711870
Step 300 - mean train loss  0.688477
Step 400 - mean train loss  0.664441
Step 500 - mean train loss  0.650610
Step 600 - mean train loss  0.657684
Step 700 - mean train loss  0.684972
Step 800 - mean train loss  0.680322
Step 900 - mean train loss  0.667061
Epoch 81 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #81 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.87730050086975,
 'training/train_loss_mean': np.float64(0.6658047550704231),
 'training/train_loss_std': np.float64(0.5303831984666451)}
Step 0 - mean train loss  1.358439
Step 100 - mean train loss  0.700464
Step 200 - mean train loss  0.645894
Step 300 - mean train loss  0.646365
Step 400 - mean train loss  0.620063
Step 500 - mean train loss  0.620968
Step 600 - mean train loss  0.623830
Step 700 - mean train loss  0.612710
Step 800 - mean train loss  0.611000
Step 900 - mean train loss  0.605262
Epoch 82 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #82 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.84282207489014,
 'training/train_loss_mean': np.float64(0.6070404335732308),
 'training/train_loss_std': np.float64(0.4853891626006861)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.503632449903689),
 'mean_qoe': np.float64(0.0009582196701922742),
 'time/evaluation': 234.71745705604553,
 'total_qoe': np.float64(4.503632449903689)}
Step 0 - mean train loss  0.613168
Step 100 - mean train loss  0.685196
Step 200 - mean train loss  0.702792
Step 300 - mean train loss  0.676830
Step 400 - mean train loss  0.646018
Step 500 - mean train loss  0.623386
Step 600 - mean train loss  0.644211
Step 700 - mean train loss  0.660116
Step 800 - mean train loss  0.722073
Step 900 - mean train loss  0.716079
Epoch 83 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #83 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.95590162277222,
 'training/train_loss_mean': np.float64(0.7155482911426319),
 'training/train_loss_std': np.float64(0.5804940966114844)}
Step 0 - mean train loss  1.556075
Step 100 - mean train loss  0.836862
Step 200 - mean train loss  0.773291
Step 300 - mean train loss  0.763825
Step 400 - mean train loss  0.723881
Step 500 - mean train loss  0.711358
Step 600 - mean train loss  0.717284
Step 700 - mean train loss  0.718872
Step 800 - mean train loss  0.715593
Step 900 - mean train loss  0.714428
Epoch 84 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #84 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.93247485160828,
 'training/train_loss_mean': np.float64(0.7103909958703546),
 'training/train_loss_std': np.float64(0.5692289714776625)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.509062030991596),
 'mean_qoe': np.float64(0.0009593749002109778),
 'time/evaluation': 236.8300929069519,
 'total_qoe': np.float64(4.509062030991596)}
Step 0 - mean train loss  0.962255
Step 100 - mean train loss  0.708981
Step 200 - mean train loss  0.709074
Step 300 - mean train loss  0.701332
Step 400 - mean train loss  0.673602
Step 500 - mean train loss  0.642267
Step 600 - mean train loss  0.660646
Step 700 - mean train loss  0.675509
Step 800 - mean train loss  0.701701
Step 900 - mean train loss  0.693639
Epoch 85 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #85 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.84564518928528,
 'training/train_loss_mean': np.float64(0.6915969899521853),
 'training/train_loss_std': np.float64(0.5621768946826468)}
Step 0 - mean train loss  1.408513
Step 100 - mean train loss  0.780735
Step 200 - mean train loss  0.706882
Step 300 - mean train loss  0.736040
Step 400 - mean train loss  0.704279
Step 500 - mean train loss  0.704543
Step 600 - mean train loss  0.725821
Step 700 - mean train loss  0.763441
Step 800 - mean train loss  0.763431
Step 900 - mean train loss  0.764956
Epoch 86 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #86 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.04076743125916,
 'training/train_loss_mean': np.float64(0.7672109849471266),
 'training/train_loss_std': np.float64(0.6215649598482773)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.516030978870628),
 'mean_qoe': np.float64(0.000960857655078857),
 'time/evaluation': 234.7577383518219,
 'total_qoe': np.float64(4.516030978870628)}
Step 0 - mean train loss  0.881467
Step 100 - mean train loss  0.823030
Step 200 - mean train loss  0.775772
Step 300 - mean train loss  0.755944
Step 400 - mean train loss  0.726835
Step 500 - mean train loss  0.690986
Step 600 - mean train loss  0.689526
Step 700 - mean train loss  0.705176
Step 800 - mean train loss  0.727618
Step 900 - mean train loss  0.717255
Epoch 87 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #87 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.84547686576843,
 'training/train_loss_mean': np.float64(0.7129967204370722),
 'training/train_loss_std': np.float64(0.56420903864556)}
Step 0 - mean train loss  1.214514
Step 100 - mean train loss  0.694515
Step 200 - mean train loss  0.649107
Step 300 - mean train loss  0.681504
Step 400 - mean train loss  0.655822
Step 500 - mean train loss  0.654439
Step 600 - mean train loss  0.669722
Step 700 - mean train loss  0.666445
Step 800 - mean train loss  0.657106
Step 900 - mean train loss  0.651146
Epoch 88 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #88 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.05895614624023,
 'training/train_loss_mean': np.float64(0.6524940352935071),
 'training/train_loss_std': np.float64(0.5381453753869407)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.501048054927857),
 'mean_qoe': np.float64(0.0009576697989208206),
 'time/evaluation': 235.50052523612976,
 'total_qoe': np.float64(4.501048054927857)}
Step 0 - mean train loss  0.554620
Step 100 - mean train loss  0.698722
Step 200 - mean train loss  0.717098
Step 300 - mean train loss  0.690868
Step 400 - mean train loss  0.662595
Step 500 - mean train loss  0.638324
Step 600 - mean train loss  0.637624
Step 700 - mean train loss  0.642225
Step 800 - mean train loss  0.646720
Step 900 - mean train loss  0.650626
Epoch 89 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #89 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.89442777633667,
 'training/train_loss_mean': np.float64(0.65453204334585),
 'training/train_loss_std': np.float64(0.5474562878287247)}
Step 0 - mean train loss  1.550280
Step 100 - mean train loss  0.836464
Step 200 - mean train loss  0.786444
Step 300 - mean train loss  0.816731
Step 400 - mean train loss  0.759459
Step 500 - mean train loss  0.753857
Step 600 - mean train loss  0.746390
Step 700 - mean train loss  0.751267
Step 800 - mean train loss  0.743547
Step 900 - mean train loss  0.738531
Epoch 90 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #90 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.91150641441345,
 'training/train_loss_mean': np.float64(0.7495129780484378),
 'training/train_loss_std': np.float64(0.6169235420246248)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.50299421832862),
 'mean_qoe': np.float64(0.000958083876240132),
 'time/evaluation': 234.73819065093994,
 'total_qoe': np.float64(4.50299421832862)}
Step 0 - mean train loss  0.760120
Step 100 - mean train loss  0.765970
Step 200 - mean train loss  0.746617
Step 300 - mean train loss  0.719422
Step 400 - mean train loss  0.689690
Step 500 - mean train loss  0.656474
Step 600 - mean train loss  0.655814
Step 700 - mean train loss  0.661591
Step 800 - mean train loss  0.664130
Step 900 - mean train loss  0.662082
Epoch 91 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #91 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.80530643463135,
 'training/train_loss_mean': np.float64(0.6624205604793056),
 'training/train_loss_std': np.float64(0.549291639511542)}
Step 0 - mean train loss  1.357732
Step 100 - mean train loss  0.801007
Step 200 - mean train loss  0.759271
Step 300 - mean train loss  0.767744
Step 400 - mean train loss  0.725752
Step 500 - mean train loss  0.714037
Step 600 - mean train loss  0.706622
Step 700 - mean train loss  0.700479
Step 800 - mean train loss  0.683946
Step 900 - mean train loss  0.674757
Epoch 92 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #92 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.00991535186768,
 'training/train_loss_mean': np.float64(0.6759294769646629),
 'training/train_loss_std': np.float64(0.5658936890200114)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.51244791306836),
 'mean_qoe': np.float64(0.0009600953006528426),
 'time/evaluation': 234.63057827949524,
 'total_qoe': np.float64(4.51244791306836)}
Step 0 - mean train loss  0.847140
Step 100 - mean train loss  0.638788
Step 200 - mean train loss  0.655578
Step 300 - mean train loss  0.654115
Step 400 - mean train loss  0.632464
Step 500 - mean train loss  0.619627
Step 600 - mean train loss  0.633100
Step 700 - mean train loss  0.648695
Step 800 - mean train loss  0.653125
Step 900 - mean train loss  0.653216
Epoch 93 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #93 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.91413426399231,
 'training/train_loss_mean': np.float64(0.66162926309019),
 'training/train_loss_std': np.float64(0.5750456064332445)}
Step 0 - mean train loss  1.626140
Step 100 - mean train loss  0.779966
Step 200 - mean train loss  0.759651
Step 300 - mean train loss  0.780583
Step 400 - mean train loss  0.732139
Step 500 - mean train loss  0.717964
Step 600 - mean train loss  0.710158
Step 700 - mean train loss  0.705414
Step 800 - mean train loss  0.693967
Step 900 - mean train loss  0.687229
Epoch 94 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #94 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.99815726280212,
 'training/train_loss_mean': np.float64(0.690199875740897),
 'training/train_loss_std': np.float64(0.5777500015624839)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.503414773413983),
 'mean_qoe': np.float64(0.0009581733560455282),
 'time/evaluation': 234.60157775878906,
 'total_qoe': np.float64(4.503414773413983)}
Step 0 - mean train loss  0.884368
Step 100 - mean train loss  0.698183
Step 200 - mean train loss  0.707030
Step 300 - mean train loss  0.695149
Step 400 - mean train loss  0.671908
Step 500 - mean train loss  0.652828
Step 600 - mean train loss  0.651527
Step 700 - mean train loss  0.656391
Step 800 - mean train loss  0.663369
Step 900 - mean train loss  0.659449
Epoch 95 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #95 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.87694120407104,
 'training/train_loss_mean': np.float64(0.6708463854010221),
 'training/train_loss_std': np.float64(0.5789283693886784)}
Step 0 - mean train loss  1.742688
Step 100 - mean train loss  0.755439
Step 200 - mean train loss  0.714453
Step 300 - mean train loss  0.764943
Step 400 - mean train loss  0.757373
Step 500 - mean train loss  0.759067
Step 600 - mean train loss  0.762563
Step 700 - mean train loss  0.756544
Step 800 - mean train loss  0.763071
Step 900 - mean train loss  0.751909
Epoch 96 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #96 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.0279221534729,
 'training/train_loss_mean': np.float64(0.7567753558996008),
 'training/train_loss_std': np.float64(0.6190347266794631)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.511414109163111),
 'mean_qoe': np.float64(0.00095987534237513),
 'time/evaluation': 234.41706585884094,
 'total_qoe': np.float64(4.511414109163111)}
Step 0 - mean train loss  0.865975
Step 100 - mean train loss  0.765919
Step 200 - mean train loss  0.755512
Step 300 - mean train loss  0.731956
Step 400 - mean train loss  0.706467
Step 500 - mean train loss  0.681027
Step 600 - mean train loss  0.691086
Step 700 - mean train loss  0.696448
Step 800 - mean train loss  0.697504
Step 900 - mean train loss  0.685279
Epoch 97 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #97 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.9345977306366,
 'training/train_loss_mean': np.float64(0.6826521952493622),
 'training/train_loss_std': np.float64(0.5884544681528524)}
Step 0 - mean train loss  1.631927
Step 100 - mean train loss  0.685121
Step 200 - mean train loss  0.653505
Step 300 - mean train loss  0.718073
Step 400 - mean train loss  0.706464
Step 500 - mean train loss  0.717080
Step 600 - mean train loss  0.733635
Step 700 - mean train loss  0.745782
Step 800 - mean train loss  0.755678
Step 900 - mean train loss  0.752177
Epoch 98 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #98 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.16673874855042,
 'training/train_loss_mean': np.float64(0.7580893140561401),
 'training/train_loss_std': np.float64(0.6361928841983971)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.500160541492313),
 'mean_qoe': np.float64(0.0009574809662749602),
 'time/evaluation': 234.7447066307068,
 'total_qoe': np.float64(4.500160541492313)}
Step 0 - mean train loss  0.902656
Step 100 - mean train loss  0.870147
Step 200 - mean train loss  0.831729
Step 300 - mean train loss  0.789057
Step 400 - mean train loss  0.740910
Step 500 - mean train loss  0.700353
Step 600 - mean train loss  0.710497
Step 700 - mean train loss  0.719067
Step 800 - mean train loss  0.742247
Step 900 - mean train loss  0.761084
Epoch 99 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #99 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.97656321525574,
 'training/train_loss_mean': np.float64(0.7610606159841589),
 'training/train_loss_std': np.float64(0.6673550726444211)}
Step 0 - mean train loss  1.837647
Step 100 - mean train loss  0.876330
Step 200 - mean train loss  0.756537
Step 300 - mean train loss  0.774548
Step 400 - mean train loss  0.730276
Step 500 - mean train loss  0.714078
Step 600 - mean train loss  0.711798
Step 700 - mean train loss  0.705910
Step 800 - mean train loss  0.699771
Step 900 - mean train loss  0.697662
Epoch 100 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #100 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.08665657043457,
 'training/train_loss_mean': np.float64(0.7045101495579188),
 'training/train_loss_std': np.float64(0.598196739022829)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.505672840581417),
 'mean_qoe': np.float64(0.0009586537958683866),
 'time/evaluation': 236.7626097202301,
 'total_qoe': np.float64(4.505672840581417)}
Step 0 - mean train loss  0.714326
Step 100 - mean train loss  0.792660
Step 200 - mean train loss  0.789530
Step 300 - mean train loss  0.744171
Step 400 - mean train loss  0.735991
Step 500 - mean train loss  0.714659
Step 600 - mean train loss  0.713267
Step 700 - mean train loss  0.707829
Step 800 - mean train loss  0.706321
Step 900 - mean train loss  0.694169
Epoch 101 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #101 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.89303159713745,
 'training/train_loss_mean': np.float64(0.6970405806276461),
 'training/train_loss_std': np.float64(0.6039814819771923)}
Step 0 - mean train loss  1.565604
Step 100 - mean train loss  0.791021
Step 200 - mean train loss  0.689580
Step 300 - mean train loss  0.701786
Step 400 - mean train loss  0.673978
Step 500 - mean train loss  0.670337
Step 600 - mean train loss  0.674787
Step 700 - mean train loss  0.678579
Step 800 - mean train loss  0.674167
Step 900 - mean train loss  0.683238
Epoch 102 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #102 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.7942578792572,
 'training/train_loss_mean': np.float64(0.692334288758641),
 'training/train_loss_std': np.float64(0.6057377133930587)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.511220481190084),
 'mean_qoe': np.float64(0.0009598341449340604),
 'time/evaluation': 234.4246950149536,
 'total_qoe': np.float64(4.511220481190084)}
Step 0 - mean train loss  1.053733
Step 100 - mean train loss  0.825023
Step 200 - mean train loss  0.788503
Step 300 - mean train loss  0.749120
Step 400 - mean train loss  0.707508
Step 500 - mean train loss  0.683046
Step 600 - mean train loss  0.691066
Step 700 - mean train loss  0.690396
Step 800 - mean train loss  0.701783
Step 900 - mean train loss  0.690744
Epoch 103 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #103 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.98148798942566,
 'training/train_loss_mean': np.float64(0.6950338313631746),
 'training/train_loss_std': np.float64(0.6266705397840422)}
Step 0 - mean train loss  1.215842
Step 100 - mean train loss  0.730371
Step 200 - mean train loss  0.651553
Step 300 - mean train loss  0.659406
Step 400 - mean train loss  0.644175
Step 500 - mean train loss  0.648354
Step 600 - mean train loss  0.660545
Step 700 - mean train loss  0.676851
Step 800 - mean train loss  0.688791
Step 900 - mean train loss  0.692110
Epoch 104 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #104 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.9221715927124,
 'training/train_loss_mean': np.float64(0.7145721441609294),
 'training/train_loss_std': np.float64(0.6215366841336111)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.503056013887575),
 'mean_qoe': np.float64(0.0009580970242313989),
 'time/evaluation': 234.48649668693542,
 'total_qoe': np.float64(4.503056013887575)}
Step 0 - mean train loss  0.863868
Step 100 - mean train loss  0.842177
Step 200 - mean train loss  0.893578
Step 300 - mean train loss  0.857704
Step 400 - mean train loss  0.818259
Step 500 - mean train loss  0.800232
Step 600 - mean train loss  0.789977
Step 700 - mean train loss  0.781273
Step 800 - mean train loss  0.788893
Step 900 - mean train loss  0.777835
Epoch 105 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #105 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.9252758026123,
 'training/train_loss_mean': np.float64(0.7758471998853032),
 'training/train_loss_std': np.float64(0.6718831390002338)}
Step 0 - mean train loss  1.406337
Step 100 - mean train loss  0.775663
Step 200 - mean train loss  0.690278
Step 300 - mean train loss  0.691635
Step 400 - mean train loss  0.668004
Step 500 - mean train loss  0.663711
Step 600 - mean train loss  0.668306
Step 700 - mean train loss  0.676224
Step 800 - mean train loss  0.690345
Step 900 - mean train loss  0.696953
Epoch 106 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #106 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.77370381355286,
 'training/train_loss_mean': np.float64(0.7065997659539687),
 'training/train_loss_std': np.float64(0.619512129091137)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.512016894844329),
 'mean_qoe': np.float64(0.0009600035946477297),
 'time/evaluation': 236.7020719051361,
 'total_qoe': np.float64(4.512016894844329)}
Step 0 - mean train loss  0.622914
Step 100 - mean train loss  0.760519
Step 200 - mean train loss  0.802809
Step 300 - mean train loss  0.766804
Step 400 - mean train loss  0.733405
Step 500 - mean train loss  0.705814
Step 600 - mean train loss  0.705411
Step 700 - mean train loss  0.702892
Step 800 - mean train loss  0.704482
Step 900 - mean train loss  0.691801
Epoch 107 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #107 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.87718462944031,
 'training/train_loss_mean': np.float64(0.6910860976209316),
 'training/train_loss_std': np.float64(0.6176152396043486)}
Step 0 - mean train loss  1.356247
Step 100 - mean train loss  0.683182
Step 200 - mean train loss  0.650014
Step 300 - mean train loss  0.661171
Step 400 - mean train loss  0.645591
Step 500 - mean train loss  0.649543
Step 600 - mean train loss  0.660716
Step 700 - mean train loss  0.665973
Step 800 - mean train loss  0.673795
Step 900 - mean train loss  0.682563
Epoch 108 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #108 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.95635914802551,
 'training/train_loss_mean': np.float64(0.69386921499163),
 'training/train_loss_std': np.float64(0.6122765976130413)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513486235448245),
 'mean_qoe': np.float64(0.0009603162203081373),
 'time/evaluation': 234.74332118034363,
 'total_qoe': np.float64(4.513486235448245)}
Step 0 - mean train loss  0.501529
Step 100 - mean train loss  0.696233
Step 200 - mean train loss  0.775095
Step 300 - mean train loss  0.802122
Step 400 - mean train loss  0.765071
Step 500 - mean train loss  0.756358
Step 600 - mean train loss  0.760147
Step 700 - mean train loss  0.759132
Step 800 - mean train loss  0.765401
Step 900 - mean train loss  0.751608
Epoch 109 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #109 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.03030490875244,
 'training/train_loss_mean': np.float64(0.7524118902433821),
 'training/train_loss_std': np.float64(0.6671829679319813)}
Step 0 - mean train loss  1.251459
Step 100 - mean train loss  0.739600
Step 200 - mean train loss  0.663057
Step 300 - mean train loss  0.665850
Step 400 - mean train loss  0.647134
Step 500 - mean train loss  0.648376
Step 600 - mean train loss  0.656363
Step 700 - mean train loss  0.654827
Step 800 - mean train loss  0.660395
Step 900 - mean train loss  0.672161
Epoch 110 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #110 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.00563097000122,
 'training/train_loss_mean': np.float64(0.6896759874805001),
 'training/train_loss_std': np.float64(0.621660886083417)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.519094224034497),
 'mean_qoe': np.float64(0.0009615094093690419),
 'time/evaluation': 234.52534747123718,
 'total_qoe': np.float64(4.519094224034497)}
Step 0 - mean train loss  0.548816
Step 100 - mean train loss  0.718945
Step 200 - mean train loss  0.821507
Step 300 - mean train loss  0.831248
Step 400 - mean train loss  0.792232
Step 500 - mean train loss  0.766554
Step 600 - mean train loss  0.755368
Step 700 - mean train loss  0.753825
Step 800 - mean train loss  0.751935
Step 900 - mean train loss  0.740904
Epoch 111 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #111 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.84398102760315,
 'training/train_loss_mean': np.float64(0.7402051260369846),
 'training/train_loss_std': np.float64(0.6675253825002874)}
Step 0 - mean train loss  1.160496
Step 100 - mean train loss  0.744085
Step 200 - mean train loss  0.709420
Step 300 - mean train loss  0.742277
Step 400 - mean train loss  0.717046
Step 500 - mean train loss  0.708259
Step 600 - mean train loss  0.708125
Step 700 - mean train loss  0.697190
Step 800 - mean train loss  0.687317
Step 900 - mean train loss  0.684904
Epoch 112 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #112 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.01221752166748,
 'training/train_loss_mean': np.float64(0.6971292220749815),
 'training/train_loss_std': np.float64(0.6253153159426047)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.515772658668152),
 'mean_qoe': np.float64(0.0009608026933336494),
 'time/evaluation': 236.86416506767273,
 'total_qoe': np.float64(4.515772658668152)}
Step 0 - mean train loss  0.486583
Step 100 - mean train loss  0.677670
Step 200 - mean train loss  0.749912
Step 300 - mean train loss  0.790871
Step 400 - mean train loss  0.753161
Step 500 - mean train loss  0.750889
Step 600 - mean train loss  0.768962
Step 700 - mean train loss  0.763415
Step 800 - mean train loss  0.769910
Step 900 - mean train loss  0.760623
Epoch 113 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #113 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.83619952201843,
 'training/train_loss_mean': np.float64(0.7647603173083597),
 'training/train_loss_std': np.float64(0.6930713456050889)}
Step 0 - mean train loss  1.269624
Step 100 - mean train loss  0.776519
Step 200 - mean train loss  0.747597
Step 300 - mean train loss  0.753642
Step 400 - mean train loss  0.735390
Step 500 - mean train loss  0.746526
Step 600 - mean train loss  0.743814
Step 700 - mean train loss  0.738046
Step 800 - mean train loss  0.723723
Step 900 - mean train loss  0.731723
Epoch 114 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #114 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.77796196937561,
 'training/train_loss_mean': np.float64(0.7365858226820378),
 'training/train_loss_std': np.float64(0.6713253047919165)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513969888104458),
 'mean_qoe': np.float64(0.000960419125128608),
 'time/evaluation': 234.00371289253235,
 'total_qoe': np.float64(4.513969888104458)}
Step 0 - mean train loss  0.257935
Step 100 - mean train loss  0.850675
Step 200 - mean train loss  0.827559
Step 300 - mean train loss  0.819922
Step 400 - mean train loss  0.782967
Step 500 - mean train loss  0.758411
Step 600 - mean train loss  0.784636
Step 700 - mean train loss  0.785919
Step 800 - mean train loss  0.800668
Step 900 - mean train loss  0.814565
Epoch 115 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #115 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.27175283432007,
 'training/train_loss_mean': np.float64(0.8230506532498141),
 'training/train_loss_std': np.float64(0.72537778895093)}
Step 0 - mean train loss  1.473824
Step 100 - mean train loss  0.803119
Step 200 - mean train loss  0.719855
Step 300 - mean train loss  0.717796
Step 400 - mean train loss  0.696427
Step 500 - mean train loss  0.705041
Step 600 - mean train loss  0.700760
Step 700 - mean train loss  0.691377
Step 800 - mean train loss  0.678635
Step 900 - mean train loss  0.683311
Epoch 116 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #116 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.84758925437927,
 'training/train_loss_mean': np.float64(0.6912238797947823),
 'training/train_loss_std': np.float64(0.6384395126134798)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.518491845350671),
 'mean_qoe': np.float64(0.0009613812436916322),
 'time/evaluation': 234.13708972930908,
 'total_qoe': np.float64(4.518491845350671)}
Step 0 - mean train loss  0.565217
Step 100 - mean train loss  0.704791
Step 200 - mean train loss  0.746993
Step 300 - mean train loss  0.769492
Step 400 - mean train loss  0.730440
Step 500 - mean train loss  0.732168
Step 600 - mean train loss  0.752950
Step 700 - mean train loss  0.745343
Step 800 - mean train loss  0.741770
Step 900 - mean train loss  0.725089
Epoch 117 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #117 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.70095252990723,
 'training/train_loss_mean': np.float64(0.7295611014500478),
 'training/train_loss_std': np.float64(0.661243373453982)}
Step 0 - mean train loss  1.096986
Step 100 - mean train loss  0.749289
Step 200 - mean train loss  0.695128
Step 300 - mean train loss  0.716530
Step 400 - mean train loss  0.688707
Step 500 - mean train loss  0.689758
Step 600 - mean train loss  0.688725
Step 700 - mean train loss  0.682695
Step 800 - mean train loss  0.671406
Step 900 - mean train loss  0.670606
Epoch 118 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #118 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.88145136833191,
 'training/train_loss_mean': np.float64(0.6766256586476911),
 'training/train_loss_std': np.float64(0.6371206725993991)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.519502043628317),
 'mean_qoe': np.float64(0.0009615961794953867),
 'time/evaluation': 236.50441455841064,
 'total_qoe': np.float64(4.519502043628317)}
Step 0 - mean train loss  0.375995
Step 100 - mean train loss  0.692716
Step 200 - mean train loss  0.708542
Step 300 - mean train loss  0.712918
Step 400 - mean train loss  0.684445
Step 500 - mean train loss  0.678432
Step 600 - mean train loss  0.694815
Step 700 - mean train loss  0.695044
Step 800 - mean train loss  0.695580
Step 900 - mean train loss  0.680990
Epoch 119 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #119 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.86250352859497,
 'training/train_loss_mean': np.float64(0.6852286464393803),
 'training/train_loss_std': np.float64(0.6433539747083534)}
Checkpoint saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/119
Step 0 - mean train loss  1.512202
Step 100 - mean train loss  0.711691
Step 200 - mean train loss  0.653084
Step 300 - mean train loss  0.664453
Step 400 - mean train loss  0.641951
Step 500 - mean train loss  0.646619
Step 600 - mean train loss  0.653942
Step 700 - mean train loss  0.656803
Step 800 - mean train loss  0.651534
Step 900 - mean train loss  0.678363
Epoch 120 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #120 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.869313955307,
 'training/train_loss_mean': np.float64(0.6923478777912515),
 'training/train_loss_std': np.float64(0.6669619198041505)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.514994146642567),
 'mean_qoe': np.float64(0.000960637052477142),
 'time/evaluation': 233.80367183685303,
 'total_qoe': np.float64(4.514994146642567)}
Step 0 - mean train loss  0.510655
Step 100 - mean train loss  0.751722
Step 200 - mean train loss  0.796806
Step 300 - mean train loss  0.792786
Step 400 - mean train loss  0.784229
Step 500 - mean train loss  0.752846
Step 600 - mean train loss  0.778565
Step 700 - mean train loss  0.770520
Step 800 - mean train loss  0.776745
Step 900 - mean train loss  0.774278
Epoch 121 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #121 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.88963055610657,
 'training/train_loss_mean': np.float64(0.7751319205923779),
 'training/train_loss_std': np.float64(0.7165142593179754)}
Step 0 - mean train loss  1.211954
Step 100 - mean train loss  0.975528
Step 200 - mean train loss  0.821513
Step 300 - mean train loss  0.785845
Step 400 - mean train loss  0.745167
Step 500 - mean train loss  0.732346
Step 600 - mean train loss  0.721819
Step 700 - mean train loss  0.709319
Step 800 - mean train loss  0.699738
Step 900 - mean train loss  0.704714
Epoch 122 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #122 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.18798065185547,
 'training/train_loss_mean': np.float64(0.7089835593419812),
 'training/train_loss_std': np.float64(0.6759117700024466)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.512162952652389),
 'mean_qoe': np.float64(0.000960034670777104),
 'time/evaluation': 233.959778547287,
 'total_qoe': np.float64(4.512162952652389)}
Step 0 - mean train loss  0.686625
Step 100 - mean train loss  0.689562
Step 200 - mean train loss  0.714588
Step 300 - mean train loss  0.726562
Step 400 - mean train loss  0.705328
Step 500 - mean train loss  0.689461
Step 600 - mean train loss  0.715363
Step 700 - mean train loss  0.713665
Step 800 - mean train loss  0.720349
Step 900 - mean train loss  0.703326
Epoch 123 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #123 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.7615852355957,
 'training/train_loss_mean': np.float64(0.7053813191955487),
 'training/train_loss_std': np.float64(0.6792339901135336)}
Step 0 - mean train loss  1.405006
Step 100 - mean train loss  0.784326
Step 200 - mean train loss  0.698053
Step 300 - mean train loss  0.690727
Step 400 - mean train loss  0.678909
Step 500 - mean train loss  0.692667
Step 600 - mean train loss  0.696066
Step 700 - mean train loss  0.700698
Step 800 - mean train loss  0.699926
Step 900 - mean train loss  0.698869
Epoch 124 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #124 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.73821568489075,
 'training/train_loss_mean': np.float64(0.7036956554848367),
 'training/train_loss_std': np.float64(0.6887267137581519)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.512713815281457),
 'mean_qoe': np.float64(0.0009601518755917994),
 'time/evaluation': 236.42509627342224,
 'total_qoe': np.float64(4.512713815281457)}
Step 0 - mean train loss  0.742373
Step 100 - mean train loss  0.672366
Step 200 - mean train loss  0.700219
Step 300 - mean train loss  0.700187
Step 400 - mean train loss  0.691134
Step 500 - mean train loss  0.672883
Step 600 - mean train loss  0.690104
Step 700 - mean train loss  0.689019
Step 800 - mean train loss  0.692263
Step 900 - mean train loss  0.682346
Epoch 125 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #125 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.76691579818726,
 'training/train_loss_mean': np.float64(0.6864537678211783),
 'training/train_loss_std': np.float64(0.6642426925383852)}
Step 0 - mean train loss  1.370468
Step 100 - mean train loss  0.775188
Step 200 - mean train loss  0.698938
Step 300 - mean train loss  0.700237
Step 400 - mean train loss  0.678697
Step 500 - mean train loss  0.684362
Step 600 - mean train loss  0.690210
Step 700 - mean train loss  0.693652
Step 800 - mean train loss  0.684017
Step 900 - mean train loss  0.687622
Epoch 126 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #126 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.45920896530151,
 'training/train_loss_mean': np.float64(0.6987100231394268),
 'training/train_loss_std': np.float64(0.6963759249675083)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513950727350439),
 'mean_qoe': np.float64(0.0009604150483724338),
 'time/evaluation': 233.77511882781982,
 'total_qoe': np.float64(4.513950727350439)}
Step 0 - mean train loss  0.642791
Step 100 - mean train loss  0.699523
Step 200 - mean train loss  0.708894
Step 300 - mean train loss  0.714244
Step 400 - mean train loss  0.692909
Step 500 - mean train loss  0.669499
Step 600 - mean train loss  0.684300
Step 700 - mean train loss  0.692578
Step 800 - mean train loss  0.697192
Step 900 - mean train loss  0.687769
Epoch 127 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #127 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.23509192466736,
 'training/train_loss_mean': np.float64(0.7013501147118064),
 'training/train_loss_std': np.float64(0.6807133344489479)}
Step 0 - mean train loss  1.555735
Step 100 - mean train loss  0.847598
Step 200 - mean train loss  0.741572
Step 300 - mean train loss  0.723754
Step 400 - mean train loss  0.688386
Step 500 - mean train loss  0.680653
Step 600 - mean train loss  0.671701
Step 700 - mean train loss  0.671063
Step 800 - mean train loss  0.664325
Step 900 - mean train loss  0.667435
Epoch 128 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #128 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.03591513633728,
 'training/train_loss_mean': np.float64(0.6811987560830745),
 'training/train_loss_std': np.float64(0.6683655699778537)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5171977371676615),
 'mean_qoe': np.float64(0.0009611059015250344),
 'time/evaluation': 233.87859988212585,
 'total_qoe': np.float64(4.5171977371676615)}
Step 0 - mean train loss  0.350699
Step 100 - mean train loss  0.670187
Step 200 - mean train loss  0.698091
Step 300 - mean train loss  0.738235
Step 400 - mean train loss  0.746148
Step 500 - mean train loss  0.730118
Step 600 - mean train loss  0.729647
Step 700 - mean train loss  0.726966
Step 800 - mean train loss  0.721531
Step 900 - mean train loss  0.710331
Epoch 129 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #129 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.75965619087219,
 'training/train_loss_mean': np.float64(0.708914439333384),
 'training/train_loss_std': np.float64(0.6778100780823252)}
Step 0 - mean train loss  1.440468
Step 100 - mean train loss  0.817184
Step 200 - mean train loss  0.738114
Step 300 - mean train loss  0.742656
Step 400 - mean train loss  0.713874
Step 500 - mean train loss  0.719332
Step 600 - mean train loss  0.711251
Step 700 - mean train loss  0.708581
Step 800 - mean train loss  0.696393
Step 900 - mean train loss  0.687093
Epoch 130 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #130 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.90363001823425,
 'training/train_loss_mean': np.float64(0.6904935782632605),
 'training/train_loss_std': np.float64(0.6848742426979512)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.51676395407208),
 'mean_qoe': np.float64(0.0009610136072493787),
 'time/evaluation': 236.32556772232056,
 'total_qoe': np.float64(4.51676395407208)}
Step 0 - mean train loss  0.535770
Step 100 - mean train loss  0.662943
Step 200 - mean train loss  0.690399
Step 300 - mean train loss  0.714762
Step 400 - mean train loss  0.710908
Step 500 - mean train loss  0.700917
Step 600 - mean train loss  0.708016
Step 700 - mean train loss  0.708238
Step 800 - mean train loss  0.707715
Step 900 - mean train loss  0.689391
Epoch 131 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #131 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.68935585021973,
 'training/train_loss_mean': np.float64(0.6914454450744182),
 'training/train_loss_std': np.float64(0.6774079877861349)}
Step 0 - mean train loss  1.377533
Step 100 - mean train loss  0.758501
Step 200 - mean train loss  0.685330
Step 300 - mean train loss  0.691670
Step 400 - mean train loss  0.674927
Step 500 - mean train loss  0.672959
Step 600 - mean train loss  0.664770
Step 700 - mean train loss  0.664025
Step 800 - mean train loss  0.660679
Step 900 - mean train loss  0.662463
Epoch 132 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #132 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.7884247303009,
 'training/train_loss_mean': np.float64(0.6675841130365688),
 'training/train_loss_std': np.float64(0.6700982542175815)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.519273575012249),
 'mean_qoe': np.float64(0.0009615475691515423),
 'time/evaluation': 234.1495270729065,
 'total_qoe': np.float64(4.519273575012249)}
Step 0 - mean train loss  0.531028
Step 100 - mean train loss  0.612041
Step 200 - mean train loss  0.656872
Step 300 - mean train loss  0.676608
Step 400 - mean train loss  0.675044
Step 500 - mean train loss  0.670529
Step 600 - mean train loss  0.688787
Step 700 - mean train loss  0.699272
Step 800 - mean train loss  0.710138
Step 900 - mean train loss  0.698157
Epoch 133 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #133 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.77380657196045,
 'training/train_loss_mean': np.float64(0.7067417042677393),
 'training/train_loss_std': np.float64(0.6868748521711175)}
Step 0 - mean train loss  1.360172
Step 100 - mean train loss  0.825613
Step 200 - mean train loss  0.748015
Step 300 - mean train loss  0.756422
Step 400 - mean train loss  0.726379
Step 500 - mean train loss  0.731804
Step 600 - mean train loss  0.720396
Step 700 - mean train loss  0.717866
Step 800 - mean train loss  0.707274
Step 900 - mean train loss  0.702091
Epoch 134 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #134 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.21248412132263,
 'training/train_loss_mean': np.float64(0.706159500708465),
 'training/train_loss_std': np.float64(0.7089332930441725)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5161120803385755),
 'mean_qoe': np.float64(0.0009608749107103352),
 'time/evaluation': 234.04614925384521,
 'total_qoe': np.float64(4.5161120803385755)}
Step 0 - mean train loss  0.424385
Step 100 - mean train loss  0.662784
Step 200 - mean train loss  0.675210
Step 300 - mean train loss  0.669548
Step 400 - mean train loss  0.658595
Step 500 - mean train loss  0.656087
Step 600 - mean train loss  0.671023
Step 700 - mean train loss  0.675934
Step 800 - mean train loss  0.684232
Step 900 - mean train loss  0.677256
Epoch 135 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #135 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.66930103302002,
 'training/train_loss_mean': np.float64(0.6823671250189579),
 'training/train_loss_std': np.float64(0.6715946836635616)}
Step 0 - mean train loss  1.539501
Step 100 - mean train loss  0.836786
Step 200 - mean train loss  0.758733
Step 300 - mean train loss  0.762488
Step 400 - mean train loss  0.745491
Step 500 - mean train loss  0.758348
Step 600 - mean train loss  0.737617
Step 700 - mean train loss  0.724565
Step 800 - mean train loss  0.714292
Step 900 - mean train loss  0.710479
Epoch 136 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #136 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.7040536403656,
 'training/train_loss_mean': np.float64(0.7157240451982009),
 'training/train_loss_std': np.float64(0.7219600454266158)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.50414874768782),
 'mean_qoe': np.float64(0.0009583295207846426),
 'time/evaluation': 233.9971079826355,
 'total_qoe': np.float64(4.50414874768782)}
Step 0 - mean train loss  0.905708
Step 100 - mean train loss  0.695309
Step 200 - mean train loss  0.710062
Step 300 - mean train loss  0.707099
Step 400 - mean train loss  0.695072
Step 500 - mean train loss  0.677952
Step 600 - mean train loss  0.684822
Step 700 - mean train loss  0.683405
Step 800 - mean train loss  0.684315
Step 900 - mean train loss  0.670407
Epoch 137 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #137 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.21526074409485,
 'training/train_loss_mean': np.float64(0.6743867685049156),
 'training/train_loss_std': np.float64(0.6819438554931676)}
Step 0 - mean train loss  1.221166
Step 100 - mean train loss  0.719262
Step 200 - mean train loss  0.669775
Step 300 - mean train loss  0.685136
Step 400 - mean train loss  0.663270
Step 500 - mean train loss  0.662251
Step 600 - mean train loss  0.654844
Step 700 - mean train loss  0.650241
Step 800 - mean train loss  0.642124
Step 900 - mean train loss  0.639813
Epoch 138 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #138 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.82040882110596,
 'training/train_loss_mean': np.float64(0.6433902444438049),
 'training/train_loss_std': np.float64(0.6543005692661619)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.514448248441385),
 'mean_qoe': np.float64(0.0009605209039236989),
 'time/evaluation': 234.18494820594788,
 'total_qoe': np.float64(4.514448248441385)}
Step 0 - mean train loss  1.138606
Step 100 - mean train loss  0.744827
Step 200 - mean train loss  0.707979
Step 300 - mean train loss  0.701228
Step 400 - mean train loss  0.682933
Step 500 - mean train loss  0.668308
Step 600 - mean train loss  0.681991
Step 700 - mean train loss  0.692888
Step 800 - mean train loss  0.691975
Step 900 - mean train loss  0.677945
Epoch 139 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #139 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.80246376991272,
 'training/train_loss_mean': np.float64(0.6766451446486694),
 'training/train_loss_std': np.float64(0.6920090462839011)}
Step 0 - mean train loss  1.377449
Step 100 - mean train loss  0.759165
Step 200 - mean train loss  0.686137
Step 300 - mean train loss  0.698849
Step 400 - mean train loss  0.668893
Step 500 - mean train loss  0.663302
Step 600 - mean train loss  0.660955
Step 700 - mean train loss  0.657492
Step 800 - mean train loss  0.647784
Step 900 - mean train loss  0.647484
Epoch 140 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #140 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.850825548172,
 'training/train_loss_mean': np.float64(0.6510163362164316),
 'training/train_loss_std': np.float64(0.6734371916564593)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513691518298397),
 'mean_qoe': np.float64(0.0009603598975102971),
 'time/evaluation': 236.44521808624268,
 'total_qoe': np.float64(4.513691518298397)}
Step 0 - mean train loss  1.004097
Step 100 - mean train loss  0.664061
Step 200 - mean train loss  0.646577
Step 300 - mean train loss  0.645656
Step 400 - mean train loss  0.631833
Step 500 - mean train loss  0.617753
Step 600 - mean train loss  0.629236
Step 700 - mean train loss  0.637440
Step 800 - mean train loss  0.642829
Step 900 - mean train loss  0.632381
Epoch 141 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #141 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.7587239742279,
 'training/train_loss_mean': np.float64(0.6354132382103858),
 'training/train_loss_std': np.float64(0.6574731077707395)}
Step 0 - mean train loss  1.291747
Step 100 - mean train loss  0.716458
Step 200 - mean train loss  0.686843
Step 300 - mean train loss  0.701256
Step 400 - mean train loss  0.661589
Step 500 - mean train loss  0.651202
Step 600 - mean train loss  0.644198
Step 700 - mean train loss  0.643784
Step 800 - mean train loss  0.639243
Step 900 - mean train loss  0.638888
Epoch 142 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #142 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.84463572502136,
 'training/train_loss_mean': np.float64(0.6447616395500964),
 'training/train_loss_std': np.float64(0.6801252247927314)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513984623537404),
 'mean_qoe': np.float64(0.0009604222603271071),
 'time/evaluation': 233.86984038352966,
 'total_qoe': np.float64(4.513984623537404)}
Step 0 - mean train loss  0.971421
Step 100 - mean train loss  0.741154
Step 200 - mean train loss  0.698692
Step 300 - mean train loss  0.694606
Step 400 - mean train loss  0.677647
Step 500 - mean train loss  0.654486
Step 600 - mean train loss  0.660750
Step 700 - mean train loss  0.671161
Step 800 - mean train loss  0.672774
Step 900 - mean train loss  0.660859
Epoch 143 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #143 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.90233063697815,
 'training/train_loss_mean': np.float64(0.6580411411810366),
 'training/train_loss_std': np.float64(0.6903281707383985)}
Step 0 - mean train loss  1.447246
Step 100 - mean train loss  0.675940
Step 200 - mean train loss  0.629373
Step 300 - mean train loss  0.660749
Step 400 - mean train loss  0.627515
Step 500 - mean train loss  0.618410
Step 600 - mean train loss  0.617862
Step 700 - mean train loss  0.620229
Step 800 - mean train loss  0.615304
Step 900 - mean train loss  0.619523
Epoch 144 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #144 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.36622452735901,
 'training/train_loss_mean': np.float64(0.629609506925115),
 'training/train_loss_std': np.float64(0.6703667764939284)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.515954008898421),
 'mean_qoe': np.float64(0.0009608412784890258),
 'time/evaluation': 234.23332357406616,
 'total_qoe': np.float64(4.515954008898421)}
Step 0 - mean train loss  1.363242
Step 100 - mean train loss  0.786559
Step 200 - mean train loss  0.727857
Step 300 - mean train loss  0.709855
Step 400 - mean train loss  0.691130
Step 500 - mean train loss  0.666045
Step 600 - mean train loss  0.666360
Step 700 - mean train loss  0.672045
Step 800 - mean train loss  0.671450
Step 900 - mean train loss  0.655301
Epoch 145 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #145 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.77061772346497,
 'training/train_loss_mean': np.float64(0.6621951677941332),
 'training/train_loss_std': np.float64(0.7004687256492267)}
Step 0 - mean train loss  1.643559
Step 100 - mean train loss  0.683725
Step 200 - mean train loss  0.638823
Step 300 - mean train loss  0.665944
Step 400 - mean train loss  0.641433
Step 500 - mean train loss  0.640260
Step 600 - mean train loss  0.634479
Step 700 - mean train loss  0.637106
Step 800 - mean train loss  0.633312
Step 900 - mean train loss  0.639712
Epoch 146 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #146 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.4423258304596,
 'training/train_loss_mean': np.float64(0.6491599347568169),
 'training/train_loss_std': np.float64(0.701671218036907)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.506370162040702),
 'mean_qoe': np.float64(0.0009588021621363196),
 'time/evaluation': 236.24440932273865,
 'total_qoe': np.float64(4.506370162040702)}
Step 0 - mean train loss  0.873339
Step 100 - mean train loss  0.723589
Step 200 - mean train loss  0.703436
Step 300 - mean train loss  0.691730
Step 400 - mean train loss  0.669292
Step 500 - mean train loss  0.654358
Step 600 - mean train loss  0.655771
Step 700 - mean train loss  0.664029
Step 800 - mean train loss  0.663919
Step 900 - mean train loss  0.650719
Epoch 147 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #147 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.73160338401794,
 'training/train_loss_mean': np.float64(0.6630332543199035),
 'training/train_loss_std': np.float64(0.7075696448919248)}
Step 0 - mean train loss  1.389743
Step 100 - mean train loss  0.780744
Step 200 - mean train loss  0.709208
Step 300 - mean train loss  0.727174
Step 400 - mean train loss  0.697131
Step 500 - mean train loss  0.697121
Step 600 - mean train loss  0.689442
Step 700 - mean train loss  0.681033
Step 800 - mean train loss  0.678261
Step 900 - mean train loss  0.683397
Epoch 148 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #148 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.5665774345398,
 'training/train_loss_mean': np.float64(0.689630485852821),
 'training/train_loss_std': np.float64(0.7330410487900431)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513871034555633),
 'mean_qoe': np.float64(0.0009603980924586453),
 'time/evaluation': 233.2486753463745,
 'total_qoe': np.float64(4.513871034555633)}
Step 0 - mean train loss  1.412992
Step 100 - mean train loss  0.868268
Step 200 - mean train loss  0.790390
Step 300 - mean train loss  0.774157
Step 400 - mean train loss  0.738029
Step 500 - mean train loss  0.710038
Step 600 - mean train loss  0.707446
Step 700 - mean train loss  0.705140
Step 800 - mean train loss  0.707915
Step 900 - mean train loss  0.694632
Epoch 149 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #149 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.83587598800659,
 'training/train_loss_mean': np.float64(0.7010370124355532),
 'training/train_loss_std': np.float64(0.7465423469156459)}
Step 0 - mean train loss  1.302158
Step 100 - mean train loss  0.860143
Step 200 - mean train loss  0.748757
Step 300 - mean train loss  0.760559
Step 400 - mean train loss  0.742633
Step 500 - mean train loss  0.725298
Step 600 - mean train loss  0.713204
Step 700 - mean train loss  0.712588
Step 800 - mean train loss  0.710052
Step 900 - mean train loss  0.723561
Epoch 150 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #150 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.19760584831238,
 'training/train_loss_mean': np.float64(0.7233558307519847),
 'training/train_loss_std': np.float64(0.7660387965494087)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.51454656707098),
 'mean_qoe': np.float64(0.0009605418227810595),
 'time/evaluation': 235.38281846046448,
 'total_qoe': np.float64(4.51454656707098)}
Step 0 - mean train loss  0.841793
Step 100 - mean train loss  0.746079
Step 200 - mean train loss  0.761839
Step 300 - mean train loss  0.766949
Step 400 - mean train loss  0.727061
Step 500 - mean train loss  0.694190
Step 600 - mean train loss  0.691387
Step 700 - mean train loss  0.694095
Step 800 - mean train loss  0.695706
Step 900 - mean train loss  0.678398
Epoch 151 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #151 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.17241740226746,
 'training/train_loss_mean': np.float64(0.6796959600675149),
 'training/train_loss_std': np.float64(0.7305602706139329)}
Step 0 - mean train loss  1.401403
Step 100 - mean train loss  0.734766
Step 200 - mean train loss  0.659773
Step 300 - mean train loss  0.670996
Step 400 - mean train loss  0.648556
Step 500 - mean train loss  0.644853
Step 600 - mean train loss  0.636223
Step 700 - mean train loss  0.634582
Step 800 - mean train loss  0.635340
Step 900 - mean train loss  0.648914
Epoch 152 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #152 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.96090507507324,
 'training/train_loss_mean': np.float64(0.6525328424048124),
 'training/train_loss_std': np.float64(0.7037797296616012)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.50155337898341),
 'mean_qoe': np.float64(0.0009577773146773214),
 'time/evaluation': 233.88750529289246,
 'total_qoe': np.float64(4.50155337898341)}
Step 0 - mean train loss  1.097536
Step 100 - mean train loss  0.847518
Step 200 - mean train loss  0.790861
Step 300 - mean train loss  0.767996
Step 400 - mean train loss  0.721263
Step 500 - mean train loss  0.689067
Step 600 - mean train loss  0.682828
Step 700 - mean train loss  0.678824
Step 800 - mean train loss  0.680331
Step 900 - mean train loss  0.666329
Epoch 153 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #153 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.48807096481323,
 'training/train_loss_mean': np.float64(0.6676383623939034),
 'training/train_loss_std': np.float64(0.7355455345039679)}
Step 0 - mean train loss  1.291106
Step 100 - mean train loss  0.664556
Step 200 - mean train loss  0.609913
Step 300 - mean train loss  0.627415
Step 400 - mean train loss  0.610958
Step 500 - mean train loss  0.613262
Step 600 - mean train loss  0.613613
Step 700 - mean train loss  0.615029
Step 800 - mean train loss  0.617255
Step 900 - mean train loss  0.636604
Epoch 154 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #154 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.88321590423584,
 'training/train_loss_mean': np.float64(0.6462696749985523),
 'training/train_loss_std': np.float64(0.7131301936972076)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.51451453323912),
 'mean_qoe': np.float64(0.0009605350070721531),
 'time/evaluation': 233.7664132118225,
 'total_qoe': np.float64(4.51451453323912)}
Step 0 - mean train loss  0.540437
Step 100 - mean train loss  0.707485
Step 200 - mean train loss  0.699488
Step 300 - mean train loss  0.725833
Step 400 - mean train loss  0.703131
Step 500 - mean train loss  0.670999
Step 600 - mean train loss  0.678121
Step 700 - mean train loss  0.674373
Step 800 - mean train loss  0.671137
Step 900 - mean train loss  0.657295
Epoch 155 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #155 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.12398433685303,
 'training/train_loss_mean': np.float64(0.6610276795914094),
 'training/train_loss_std': np.float64(0.7251559082491581)}
Step 0 - mean train loss  1.820436
Step 100 - mean train loss  0.697049
Step 200 - mean train loss  0.637798
Step 300 - mean train loss  0.651327
Step 400 - mean train loss  0.635436
Step 500 - mean train loss  0.634330
Step 600 - mean train loss  0.633394
Step 700 - mean train loss  0.633836
Step 800 - mean train loss  0.627546
Step 900 - mean train loss  0.642311
Epoch 156 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #156 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.7287278175354,
 'training/train_loss_mean': np.float64(0.648018088763779),
 'training/train_loss_std': np.float64(0.7309391982722676)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.520277335834177),
 'mean_qoe': np.float64(0.0009617611352838673),
 'time/evaluation': 234.41171598434448,
 'total_qoe': np.float64(4.520277335834177)}
Step 0 - mean train loss  0.740880
Step 100 - mean train loss  0.703330
Step 200 - mean train loss  0.699656
Step 300 - mean train loss  0.708375
Step 400 - mean train loss  0.690888
Step 500 - mean train loss  0.658250
Step 600 - mean train loss  0.660471
Step 700 - mean train loss  0.654134
Step 800 - mean train loss  0.649663
Step 900 - mean train loss  0.632244
Epoch 157 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #157 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.81038641929626,
 'training/train_loss_mean': np.float64(0.6332599193960406),
 'training/train_loss_std': np.float64(0.702017305680667)}
Step 0 - mean train loss  1.232900
Step 100 - mean train loss  0.639427
Step 200 - mean train loss  0.595794
Step 300 - mean train loss  0.604172
Step 400 - mean train loss  0.591331
Step 500 - mean train loss  0.593985
Step 600 - mean train loss  0.598865
Step 700 - mean train loss  0.607119
Step 800 - mean train loss  0.609750
Step 900 - mean train loss  0.626672
Epoch 158 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #158 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.73831987380981,
 'training/train_loss_mean': np.float64(0.64066631377521),
 'training/train_loss_std': np.float64(0.7254029244892957)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.515953669194922),
 'mean_qoe': np.float64(0.0009608412062116856),
 'time/evaluation': 234.19594073295593,
 'total_qoe': np.float64(4.515953669194922)}
Step 0 - mean train loss  0.575098
Step 100 - mean train loss  0.675698
Step 200 - mean train loss  0.679539
Step 300 - mean train loss  0.698616
Step 400 - mean train loss  0.698252
Step 500 - mean train loss  0.670676
Step 600 - mean train loss  0.672431
Step 700 - mean train loss  0.667777
Step 800 - mean train loss  0.669848
Step 900 - mean train loss  0.652426
Epoch 159 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #159 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.8136236667633,
 'training/train_loss_mean': np.float64(0.6515294669482452),
 'training/train_loss_std': np.float64(0.7338326141447234)}
Checkpoint saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/159
Step 0 - mean train loss  1.231940
Step 100 - mean train loss  0.644559
Step 200 - mean train loss  0.591885
Step 300 - mean train loss  0.598563
Step 400 - mean train loss  0.575847
Step 500 - mean train loss  0.570284
Step 600 - mean train loss  0.572597
Step 700 - mean train loss  0.585605
Step 800 - mean train loss  0.596698
Step 900 - mean train loss  0.612814
Epoch 160 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #160 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.58441948890686,
 'training/train_loss_mean': np.float64(0.6334958168735132),
 'training/train_loss_std': np.float64(0.7228756095810899)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.517266777802667),
 'mean_qoe': np.float64(0.0009611205910218441),
 'time/evaluation': 233.3918616771698,
 'total_qoe': np.float64(4.517266777802667)}
Step 0 - mean train loss  0.527672
Step 100 - mean train loss  0.692174
Step 200 - mean train loss  0.689318
Step 300 - mean train loss  0.715462
Step 400 - mean train loss  0.695514
Step 500 - mean train loss  0.663444
Step 600 - mean train loss  0.666870
Step 700 - mean train loss  0.659011
Step 800 - mean train loss  0.654159
Step 900 - mean train loss  0.638690
Epoch 161 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #161 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.31569147109985,
 'training/train_loss_mean': np.float64(0.6367508559843602),
 'training/train_loss_std': np.float64(0.7177542428139224)}
Step 0 - mean train loss  1.406432
Step 100 - mean train loss  0.648344
Step 200 - mean train loss  0.603700
Step 300 - mean train loss  0.614747
Step 400 - mean train loss  0.588696
Step 500 - mean train loss  0.580108
Step 600 - mean train loss  0.572115
Step 700 - mean train loss  0.574144
Step 800 - mean train loss  0.577998
Step 900 - mean train loss  0.592582
Epoch 162 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #162 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.79817605018616,
 'training/train_loss_mean': np.float64(0.6073063693209088),
 'training/train_loss_std': np.float64(0.6920972212378104)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.514029835794043),
 'mean_qoe': np.float64(0.0009604318799561793),
 'time/evaluation': 233.250497341156,
 'total_qoe': np.float64(4.514029835794043)}
Step 0 - mean train loss  0.767548
Step 100 - mean train loss  0.665831
Step 200 - mean train loss  0.675784
Step 300 - mean train loss  0.704340
Step 400 - mean train loss  0.692596
Step 500 - mean train loss  0.658857
Step 600 - mean train loss  0.671428
Step 700 - mean train loss  0.666296
Step 800 - mean train loss  0.663004
Step 900 - mean train loss  0.652947
Epoch 163 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #163 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.82607102394104,
 'training/train_loss_mean': np.float64(0.6512728596169376),
 'training/train_loss_std': np.float64(0.7338175652911434)}
Step 0 - mean train loss  1.675598
Step 100 - mean train loss  0.662677
Step 200 - mean train loss  0.621667
Step 300 - mean train loss  0.631448
Step 400 - mean train loss  0.595406
Step 500 - mean train loss  0.585093
Step 600 - mean train loss  0.578115
Step 700 - mean train loss  0.575704
Step 800 - mean train loss  0.580564
Step 900 - mean train loss  0.593670
Epoch 164 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #164 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.1093077659607,
 'training/train_loss_mean': np.float64(0.6056220594632513),
 'training/train_loss_std': np.float64(0.7134107855805334)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513881561130675),
 'mean_qoe': np.float64(0.0009604003321554628),
 'time/evaluation': 233.57119727134705,
 'total_qoe': np.float64(4.513881561130675)}
Step 0 - mean train loss  0.583618
Step 100 - mean train loss  0.727592
Step 200 - mean train loss  0.730778
Step 300 - mean train loss  0.731244
Step 400 - mean train loss  0.757459
Step 500 - mean train loss  0.719227
Step 600 - mean train loss  0.711640
Step 700 - mean train loss  0.715448
Step 800 - mean train loss  0.708425
Step 900 - mean train loss  0.686300
Epoch 165 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #165 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.67797541618347,
 'training/train_loss_mean': np.float64(0.6904257980079689),
 'training/train_loss_std': np.float64(0.7645862395806694)}
Step 0 - mean train loss  1.305085
Step 100 - mean train loss  0.691591
Step 200 - mean train loss  0.618256
Step 300 - mean train loss  0.628019
Step 400 - mean train loss  0.590822
Step 500 - mean train loss  0.577776
Step 600 - mean train loss  0.568697
Step 700 - mean train loss  0.565187
Step 800 - mean train loss  0.560171
Step 900 - mean train loss  0.571930
Epoch 166 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #166 ====================
>>>>>>>>>> Training Information:
{'time/training': 112.95194363594055,
 'training/train_loss_mean': np.float64(0.5843551059827836),
 'training/train_loss_std': np.float64(0.697614102288881)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.52086700928156),
 'mean_qoe': np.float64(0.0009618865977194809),
 'time/evaluation': 235.36666774749756,
 'total_qoe': np.float64(4.52086700928156)}
Step 0 - mean train loss  0.785586
Step 100 - mean train loss  0.672156
Step 200 - mean train loss  0.660167
Step 300 - mean train loss  0.683226
Step 400 - mean train loss  0.659353
Step 500 - mean train loss  0.624537
Step 600 - mean train loss  0.645228
Step 700 - mean train loss  0.655597
Step 800 - mean train loss  0.648255
Step 900 - mean train loss  0.635894
Epoch 167 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #167 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.06435489654541,
 'training/train_loss_mean': np.float64(0.6390055082438517),
 'training/train_loss_std': np.float64(0.7295346264590598)}
Step 0 - mean train loss  1.397249
Step 100 - mean train loss  0.645517
Step 200 - mean train loss  0.609125
Step 300 - mean train loss  0.616799
Step 400 - mean train loss  0.589630
Step 500 - mean train loss  0.585601
Step 600 - mean train loss  0.574809
Step 700 - mean train loss  0.567789
Step 800 - mean train loss  0.560957
Step 900 - mean train loss  0.572580
Epoch 168 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #168 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.021479845047,
 'training/train_loss_mean': np.float64(0.5837713488563916),
 'training/train_loss_std': np.float64(0.7029192152832042)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.517955363990479),
 'mean_qoe': np.float64(0.0009612670987213785),
 'time/evaluation': 233.5246205329895,
 'total_qoe': np.float64(4.517955363990479)}
Step 0 - mean train loss  0.746110
Step 100 - mean train loss  0.660154
Step 200 - mean train loss  0.654038
Step 300 - mean train loss  0.668590
Step 400 - mean train loss  0.673805
Step 500 - mean train loss  0.633000
Step 600 - mean train loss  0.638835
Step 700 - mean train loss  0.651933
Step 800 - mean train loss  0.647284
Step 900 - mean train loss  0.630354
Epoch 169 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #169 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.72240948677063,
 'training/train_loss_mean': np.float64(0.635408484195751),
 'training/train_loss_std': np.float64(0.7374164571720291)}
Step 0 - mean train loss  1.443743
Step 100 - mean train loss  0.693220
Step 200 - mean train loss  0.612386
Step 300 - mean train loss  0.610057
Step 400 - mean train loss  0.576521
Step 500 - mean train loss  0.564205
Step 600 - mean train loss  0.556836
Step 700 - mean train loss  0.552682
Step 800 - mean train loss  0.549561
Step 900 - mean train loss  0.565763
Epoch 170 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #170 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.3797254562378,
 'training/train_loss_mean': np.float64(0.5765324712892405),
 'training/train_loss_std': np.float64(0.7032499511165698)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.514523272488507),
 'mean_qoe': np.float64(0.0009605368664869164),
 'time/evaluation': 236.18874406814575,
 'total_qoe': np.float64(4.514523272488507)}
Step 0 - mean train loss  1.166384
Step 100 - mean train loss  0.683464
Step 200 - mean train loss  0.650037
Step 300 - mean train loss  0.646658
Step 400 - mean train loss  0.636543
Step 500 - mean train loss  0.599281
Step 600 - mean train loss  0.608682
Step 700 - mean train loss  0.622279
Step 800 - mean train loss  0.621341
Step 900 - mean train loss  0.610213
Epoch 171 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #171 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.44303941726685,
 'training/train_loss_mean': np.float64(0.6148750511818363),
 'training/train_loss_std': np.float64(0.7231037323569189)}
Step 0 - mean train loss  1.298434
Step 100 - mean train loss  0.652388
Step 200 - mean train loss  0.590675
Step 300 - mean train loss  0.586002
Step 400 - mean train loss  0.550084
Step 500 - mean train loss  0.536832
Step 600 - mean train loss  0.528508
Step 700 - mean train loss  0.525062
Step 800 - mean train loss  0.527503
Step 900 - mean train loss  0.544528
Epoch 172 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #172 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.50466108322144,
 'training/train_loss_mean': np.float64(0.5569284507472206),
 'training/train_loss_std': np.float64(0.6930581485830836)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.515363649669191),
 'mean_qoe': np.float64(0.000960715670142381),
 'time/evaluation': 233.7290163040161,
 'total_qoe': np.float64(4.515363649669191)}
Step 0 - mean train loss  0.854120
Step 100 - mean train loss  0.664834
Step 200 - mean train loss  0.642755
Step 300 - mean train loss  0.647165
Step 400 - mean train loss  0.627167
Step 500 - mean train loss  0.590751
Step 600 - mean train loss  0.592918
Step 700 - mean train loss  0.601375
Step 800 - mean train loss  0.595927
Step 900 - mean train loss  0.585544
Epoch 173 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #173 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.36771631240845,
 'training/train_loss_mean': np.float64(0.5854940711126273),
 'training/train_loss_std': np.float64(0.701265688662814)}
Step 0 - mean train loss  1.356364
Step 100 - mean train loss  0.709904
Step 200 - mean train loss  0.649740
Step 300 - mean train loss  0.626024
Step 400 - mean train loss  0.590750
Step 500 - mean train loss  0.572624
Step 600 - mean train loss  0.558490
Step 700 - mean train loss  0.551929
Step 800 - mean train loss  0.545222
Step 900 - mean train loss  0.556398
Epoch 174 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #174 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.24309968948364,
 'training/train_loss_mean': np.float64(0.5652308818907298),
 'training/train_loss_std': np.float64(0.7044092079573133)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.519074609493856),
 'mean_qoe': np.float64(0.0009615052360625226),
 'time/evaluation': 233.3440489768982,
 'total_qoe': np.float64(4.519074609493856)}
Step 0 - mean train loss  0.783308
Step 100 - mean train loss  0.623196
Step 200 - mean train loss  0.618860
Step 300 - mean train loss  0.615070
Step 400 - mean train loss  0.597105
Step 500 - mean train loss  0.568087
Step 600 - mean train loss  0.570292
Step 700 - mean train loss  0.576479
Step 800 - mean train loss  0.571406
Step 900 - mean train loss  0.565510
Epoch 175 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #175 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.74661779403687,
 'training/train_loss_mean': np.float64(0.5688936100781978),
 'training/train_loss_std': np.float64(0.6809447415578872)}
Step 0 - mean train loss  1.395385
Step 100 - mean train loss  0.694295
Step 200 - mean train loss  0.642134
Step 300 - mean train loss  0.617833
Step 400 - mean train loss  0.585111
Step 500 - mean train loss  0.573492
Step 600 - mean train loss  0.562086
Step 700 - mean train loss  0.555691
Step 800 - mean train loss  0.555669
Step 900 - mean train loss  0.565257
Epoch 176 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #176 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.16780090332031,
 'training/train_loss_mean': np.float64(0.5731443284310443),
 'training/train_loss_std': np.float64(0.7120925028406492)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.521682656027365),
 'mean_qoe': np.float64(0.0009620601395802904),
 'time/evaluation': 233.4257254600525,
 'total_qoe': np.float64(4.521682656027365)}
Step 0 - mean train loss  0.883165
Step 100 - mean train loss  0.630532
Step 200 - mean train loss  0.625005
Step 300 - mean train loss  0.619044
Step 400 - mean train loss  0.595177
Step 500 - mean train loss  0.563015
Step 600 - mean train loss  0.567702
Step 700 - mean train loss  0.570536
Step 800 - mean train loss  0.561410
Step 900 - mean train loss  0.554995
Epoch 177 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #177 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.56508994102478,
 'training/train_loss_mean': np.float64(0.5578610781468729),
 'training/train_loss_std': np.float64(0.678947375119269)}
Step 0 - mean train loss  1.610304
Step 100 - mean train loss  0.622292
Step 200 - mean train loss  0.574467
Step 300 - mean train loss  0.565242
Step 400 - mean train loss  0.542385
Step 500 - mean train loss  0.531862
Step 600 - mean train loss  0.521067
Step 700 - mean train loss  0.514019
Step 800 - mean train loss  0.509302
Step 900 - mean train loss  0.519866
Epoch 178 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #178 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.16855883598328,
 'training/train_loss_mean': np.float64(0.5282753358720617),
 'training/train_loss_std': np.float64(0.6767001204002362)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.512666613053248),
 'mean_qoe': np.float64(0.0009601418325645208),
 'time/evaluation': 233.97481894493103,
 'total_qoe': np.float64(4.512666613053248)}
Step 0 - mean train loss  0.751927
Step 100 - mean train loss  0.637382
Step 200 - mean train loss  0.632885
Step 300 - mean train loss  0.631427
Step 400 - mean train loss  0.616933
Step 500 - mean train loss  0.577261
Step 600 - mean train loss  0.577537
Step 700 - mean train loss  0.579754
Step 800 - mean train loss  0.572557
Step 900 - mean train loss  0.565731
Epoch 179 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #179 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.72356820106506,
 'training/train_loss_mean': np.float64(0.5678145891132449),
 'training/train_loss_std': np.float64(0.698134466438647)}
Step 0 - mean train loss  1.465931
Step 100 - mean train loss  0.659826
Step 200 - mean train loss  0.578038
Step 300 - mean train loss  0.571913
Step 400 - mean train loss  0.547178
Step 500 - mean train loss  0.542758
Step 600 - mean train loss  0.541145
Step 700 - mean train loss  0.536405
Step 800 - mean train loss  0.533838
Step 900 - mean train loss  0.546054
Epoch 180 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #180 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.4303994178772,
 'training/train_loss_mean': np.float64(0.5526853028326851),
 'training/train_loss_std': np.float64(0.7141943562877543)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.520479623456882),
 'mean_qoe': np.float64(0.0009618041752035918),
 'time/evaluation': 233.79985880851746,
 'total_qoe': np.float64(4.520479623456882)}
Step 0 - mean train loss  0.818920
Step 100 - mean train loss  0.681788
Step 200 - mean train loss  0.650947
Step 300 - mean train loss  0.649785
Step 400 - mean train loss  0.642738
Step 500 - mean train loss  0.598491
Step 600 - mean train loss  0.599535
Step 700 - mean train loss  0.608002
Step 800 - mean train loss  0.596809
Step 900 - mean train loss  0.586209
Epoch 181 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #181 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.1244523525238,
 'training/train_loss_mean': np.float64(0.5860900068729223),
 'training/train_loss_std': np.float64(0.7285675153424958)}
Step 0 - mean train loss  1.271497
Step 100 - mean train loss  0.613467
Step 200 - mean train loss  0.553404
Step 300 - mean train loss  0.554676
Step 400 - mean train loss  0.529080
Step 500 - mean train loss  0.519684
Step 600 - mean train loss  0.518115
Step 700 - mean train loss  0.513864
Step 800 - mean train loss  0.510181
Step 900 - mean train loss  0.522065
Epoch 182 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #182 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.60954523086548,
 'training/train_loss_mean': np.float64(0.5298971371037754),
 'training/train_loss_std': np.float64(0.7021585246874807)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5196373193680115),
 'mean_qoe': np.float64(0.000961624961567662),
 'time/evaluation': 233.59194660186768,
 'total_qoe': np.float64(4.5196373193680115)}
Step 0 - mean train loss  0.849714
Step 100 - mean train loss  0.620194
Step 200 - mean train loss  0.617806
Step 300 - mean train loss  0.615993
Step 400 - mean train loss  0.611367
Step 500 - mean train loss  0.570969
Step 600 - mean train loss  0.574139
Step 700 - mean train loss  0.581561
Step 800 - mean train loss  0.573727
Step 900 - mean train loss  0.562577
Epoch 183 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #183 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.66511154174805,
 'training/train_loss_mean': np.float64(0.5623285121182708),
 'training/train_loss_std': np.float64(0.7161352439575128)}
Step 0 - mean train loss  1.361424
Step 100 - mean train loss  0.597193
Step 200 - mean train loss  0.540725
Step 300 - mean train loss  0.541724
Step 400 - mean train loss  0.511405
Step 500 - mean train loss  0.507281
Step 600 - mean train loss  0.508519
Step 700 - mean train loss  0.506993
Step 800 - mean train loss  0.505829
Step 900 - mean train loss  0.517383
Epoch 184 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #184 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.35992407798767,
 'training/train_loss_mean': np.float64(0.5274138341301354),
 'training/train_loss_std': np.float64(0.7047034953214828)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5179349578074515),
 'mean_qoe': np.float64(0.0009612627569803088),
 'time/evaluation': 233.82016515731812,
 'total_qoe': np.float64(4.5179349578074515)}
Step 0 - mean train loss  1.088356
Step 100 - mean train loss  0.657038
Step 200 - mean train loss  0.640166
Step 300 - mean train loss  0.635223
Step 400 - mean train loss  0.617874
Step 500 - mean train loss  0.580555
Step 600 - mean train loss  0.586583
Step 700 - mean train loss  0.591634
Step 800 - mean train loss  0.584630
Step 900 - mean train loss  0.571028
Epoch 185 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #185 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.66022396087646,
 'training/train_loss_mean': np.float64(0.5709118993998279),
 'training/train_loss_std': np.float64(0.729359998494317)}
Step 0 - mean train loss  1.354501
Step 100 - mean train loss  0.608161
Step 200 - mean train loss  0.554009
Step 300 - mean train loss  0.553065
Step 400 - mean train loss  0.522819
Step 500 - mean train loss  0.512512
Step 600 - mean train loss  0.505879
Step 700 - mean train loss  0.502540
Step 800 - mean train loss  0.496813
Step 900 - mean train loss  0.507882
Epoch 186 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #186 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.55734968185425,
 'training/train_loss_mean': np.float64(0.5188956225144181),
 'training/train_loss_std': np.float64(0.6967576650460295)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5230202582368495),
 'mean_qoe': np.float64(0.0009623447357950744),
 'time/evaluation': 233.47917795181274,
 'total_qoe': np.float64(4.5230202582368495)}
Step 0 - mean train loss  1.242347
Step 100 - mean train loss  0.668404
Step 200 - mean train loss  0.638057
Step 300 - mean train loss  0.625783
Step 400 - mean train loss  0.605055
Step 500 - mean train loss  0.566676
Step 600 - mean train loss  0.578157
Step 700 - mean train loss  0.584061
Step 800 - mean train loss  0.578942
Step 900 - mean train loss  0.571471
Epoch 187 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #187 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.26666498184204,
 'training/train_loss_mean': np.float64(0.5694727848896177),
 'training/train_loss_std': np.float64(0.7324657128602322)}
Step 0 - mean train loss  1.225873
Step 100 - mean train loss  0.621028
Step 200 - mean train loss  0.553836
Step 300 - mean train loss  0.548972
Step 400 - mean train loss  0.510992
Step 500 - mean train loss  0.507393
Step 600 - mean train loss  0.505268
Step 700 - mean train loss  0.504368
Step 800 - mean train loss  0.501603
Step 900 - mean train loss  0.509778
Epoch 188 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #188 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.70508146286011,
 'training/train_loss_mean': np.float64(0.5167157022847809),
 'training/train_loss_std': np.float64(0.697151902247446)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.521245191164052),
 'mean_qoe': np.float64(0.0009619670619497984),
 'time/evaluation': 233.9904100894928,
 'total_qoe': np.float64(4.521245191164052)}
Step 0 - mean train loss  0.972630
Step 100 - mean train loss  0.641801
Step 200 - mean train loss  0.603807
Step 300 - mean train loss  0.590352
Step 400 - mean train loss  0.571666
Step 500 - mean train loss  0.538395
Step 600 - mean train loss  0.546872
Step 700 - mean train loss  0.554512
Step 800 - mean train loss  0.549092
Step 900 - mean train loss  0.540927
Epoch 189 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #189 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.46745014190674,
 'training/train_loss_mean': np.float64(0.5409534227663908),
 'training/train_loss_std': np.float64(0.7003594633985694)}
Step 0 - mean train loss  1.837363
Step 100 - mean train loss  0.629008
Step 200 - mean train loss  0.565340
Step 300 - mean train loss  0.567107
Step 400 - mean train loss  0.525894
Step 500 - mean train loss  0.521363
Step 600 - mean train loss  0.518395
Step 700 - mean train loss  0.516791
Step 800 - mean train loss  0.513221
Step 900 - mean train loss  0.521686
Epoch 190 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #190 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.35981559753418,
 'training/train_loss_mean': np.float64(0.5269823750886656),
 'training/train_loss_std': np.float64(0.7162204198729007)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.516876769265845),
 'mean_qoe': np.float64(0.0009610376104820947),
 'time/evaluation': 233.28817009925842,
 'total_qoe': np.float64(4.516876769265845)}
Step 0 - mean train loss  0.872573
Step 100 - mean train loss  0.630063
Step 200 - mean train loss  0.617359
Step 300 - mean train loss  0.609973
Step 400 - mean train loss  0.594537
Step 500 - mean train loss  0.555666
Step 600 - mean train loss  0.562491
Step 700 - mean train loss  0.568097
Step 800 - mean train loss  0.561235
Step 900 - mean train loss  0.552969
Epoch 191 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #191 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.2676351070404,
 'training/train_loss_mean': np.float64(0.5518326860435648),
 'training/train_loss_std': np.float64(0.723971919155652)}
Step 0 - mean train loss  1.599720
Step 100 - mean train loss  0.578815
Step 200 - mean train loss  0.534516
Step 300 - mean train loss  0.546945
Step 400 - mean train loss  0.510419
Step 500 - mean train loss  0.506461
Step 600 - mean train loss  0.504233
Step 700 - mean train loss  0.504972
Step 800 - mean train loss  0.503369
Step 900 - mean train loss  0.519616
Epoch 192 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #192 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.3097767829895,
 'training/train_loss_mean': np.float64(0.5312932588960597),
 'training/train_loss_std': np.float64(0.7276081427585821)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.519451715947308),
 'mean_qoe': np.float64(0.0009615854714781506),
 'time/evaluation': 234.45473170280457,
 'total_qoe': np.float64(4.519451715947308)}
Step 0 - mean train loss  1.111622
Step 100 - mean train loss  0.662820
Step 200 - mean train loss  0.638656
Step 300 - mean train loss  0.625974
Step 400 - mean train loss  0.603924
Step 500 - mean train loss  0.565964
Step 600 - mean train loss  0.571623
Step 700 - mean train loss  0.574529
Step 800 - mean train loss  0.568516
Step 900 - mean train loss  0.557162
Epoch 193 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #193 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.84731316566467,
 'training/train_loss_mean': np.float64(0.5537301254268949),
 'training/train_loss_std': np.float64(0.7313401561408409)}
Step 0 - mean train loss  1.437120
Step 100 - mean train loss  0.592974
Step 200 - mean train loss  0.524703
Step 300 - mean train loss  0.536155
Step 400 - mean train loss  0.505136
Step 500 - mean train loss  0.498439
Step 600 - mean train loss  0.500167
Step 700 - mean train loss  0.498751
Step 800 - mean train loss  0.496080
Step 900 - mean train loss  0.513613
Epoch 194 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #194 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.88292455673218,
 'training/train_loss_mean': np.float64(0.5253048439010481),
 'training/train_loss_std': np.float64(0.7271072055885889)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.516558085428644),
 'mean_qoe': np.float64(0.0009609698054103497),
 'time/evaluation': 234.31527185440063,
 'total_qoe': np.float64(4.516558085428644)}
Step 0 - mean train loss  1.183451
Step 100 - mean train loss  0.638783
Step 200 - mean train loss  0.622809
Step 300 - mean train loss  0.607641
Step 400 - mean train loss  0.589024
Step 500 - mean train loss  0.553435
Step 600 - mean train loss  0.559544
Step 700 - mean train loss  0.563177
Step 800 - mean train loss  0.558665
Step 900 - mean train loss  0.550875
Epoch 195 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #195 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.78931999206543,
 'training/train_loss_mean': np.float64(0.5470908452767983),
 'training/train_loss_std': np.float64(0.7287528983596449)}
Step 0 - mean train loss  1.393529
Step 100 - mean train loss  0.596205
Step 200 - mean train loss  0.530181
Step 300 - mean train loss  0.545037
Step 400 - mean train loss  0.508790
Step 500 - mean train loss  0.500532
Step 600 - mean train loss  0.497653
Step 700 - mean train loss  0.496031
Step 800 - mean train loss  0.490627
Step 900 - mean train loss  0.505082
Epoch 196 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #196 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.27865481376648,
 'training/train_loss_mean': np.float64(0.5148025585158175),
 'training/train_loss_std': np.float64(0.7290436749123024)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.518454294870609),
 'mean_qoe': np.float64(0.0009613732542277892),
 'time/evaluation': 234.1659712791443,
 'total_qoe': np.float64(4.518454294870609)}
Step 0 - mean train loss  1.445590
Step 100 - mean train loss  0.669519
Step 200 - mean train loss  0.638036
Step 300 - mean train loss  0.617694
Step 400 - mean train loss  0.603712
Step 500 - mean train loss  0.561551
Step 600 - mean train loss  0.567336
Step 700 - mean train loss  0.568748
Step 800 - mean train loss  0.563279
Step 900 - mean train loss  0.555560
Epoch 197 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #197 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.35898065567017,
 'training/train_loss_mean': np.float64(0.5490069932751096),
 'training/train_loss_std': np.float64(0.7383497724179748)}
Step 0 - mean train loss  1.106567
Step 100 - mean train loss  0.557827
Step 200 - mean train loss  0.499362
Step 300 - mean train loss  0.519258
Step 400 - mean train loss  0.492257
Step 500 - mean train loss  0.487348
Step 600 - mean train loss  0.485354
Step 700 - mean train loss  0.482213
Step 800 - mean train loss  0.476034
Step 900 - mean train loss  0.488912
Epoch 198 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #198 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.75037813186646,
 'training/train_loss_mean': np.float64(0.4959906321487744),
 'training/train_loss_std': np.float64(0.711610710264013)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.514967534319376),
 'mean_qoe': np.float64(0.0009606313902807182),
 'time/evaluation': 237.08232164382935,
 'total_qoe': np.float64(4.514967534319376)}
Step 0 - mean train loss  1.020274
Step 100 - mean train loss  0.609778
Step 200 - mean train loss  0.584434
Step 300 - mean train loss  0.577651
Step 400 - mean train loss  0.557066
Step 500 - mean train loss  0.517168
Step 600 - mean train loss  0.531664
Step 700 - mean train loss  0.538668
Step 800 - mean train loss  0.534006
Step 900 - mean train loss  0.528915
Epoch 199 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #199 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.76925921440125,
 'training/train_loss_mean': np.float64(0.52532012141235),
 'training/train_loss_std': np.float64(0.7159738110982768)}
Checkpoint saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/199
Step 0 - mean train loss  1.348492
Step 100 - mean train loss  0.555273
Step 200 - mean train loss  0.504322
Step 300 - mean train loss  0.511509
Step 400 - mean train loss  0.484158
Step 500 - mean train loss  0.483285
Step 600 - mean train loss  0.479801
Step 700 - mean train loss  0.479734
Step 800 - mean train loss  0.473148
Step 900 - mean train loss  0.485764
Epoch 200 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #200 ====================
>>>>>>>>>> Training Information:
{'time/training': 112.74645972251892,
 'training/train_loss_mean': np.float64(0.49504188494341334),
 'training/train_loss_std': np.float64(0.7163124772347155)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513459878261074),
 'mean_qoe': np.float64(0.0009603106123959732),
 'time/evaluation': 236.1088056564331,
 'total_qoe': np.float64(4.513459878261074)}
Step 0 - mean train loss  1.596948
Step 100 - mean train loss  0.643780
Step 200 - mean train loss  0.591289
Step 300 - mean train loss  0.577264
Step 400 - mean train loss  0.550019
Step 500 - mean train loss  0.510381
Step 600 - mean train loss  0.523601
Step 700 - mean train loss  0.532379
Step 800 - mean train loss  0.530817
Step 900 - mean train loss  0.523710
Epoch 201 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #201 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.91691446304321,
 'training/train_loss_mean': np.float64(0.5204214745098832),
 'training/train_loss_std': np.float64(0.721097175697595)}
Step 0 - mean train loss  1.353501
Step 100 - mean train loss  0.545411
Step 200 - mean train loss  0.487162
Step 300 - mean train loss  0.496286
Step 400 - mean train loss  0.468109
Step 500 - mean train loss  0.464375
Step 600 - mean train loss  0.463215
Step 700 - mean train loss  0.467638
Step 800 - mean train loss  0.462438
Step 900 - mean train loss  0.473082
Epoch 202 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #202 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.81696391105652,
 'training/train_loss_mean': np.float64(0.4820494573550294),
 'training/train_loss_std': np.float64(0.7064858034836514)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.511424137540144),
 'mean_qoe': np.float64(0.000959877476072371),
 'time/evaluation': 234.3250675201416,
 'total_qoe': np.float64(4.511424137540144)}
Step 0 - mean train loss  1.614480
Step 100 - mean train loss  0.635517
Step 200 - mean train loss  0.592494
Step 300 - mean train loss  0.567274
Step 400 - mean train loss  0.545169
Step 500 - mean train loss  0.505107
Step 600 - mean train loss  0.515554
Step 700 - mean train loss  0.527160
Step 800 - mean train loss  0.529294
Step 900 - mean train loss  0.522588
Epoch 203 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #203 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.81503009796143,
 'training/train_loss_mean': np.float64(0.5180795091058247),
 'training/train_loss_std': np.float64(0.7312364667101379)}
Step 0 - mean train loss  1.554250
Step 100 - mean train loss  0.529215
Step 200 - mean train loss  0.481050
Step 300 - mean train loss  0.491643
Step 400 - mean train loss  0.468776
Step 500 - mean train loss  0.461125
Step 600 - mean train loss  0.455972
Step 700 - mean train loss  0.455162
Step 800 - mean train loss  0.448756
Step 900 - mean train loss  0.457279
Epoch 204 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #204 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.08028030395508,
 'training/train_loss_mean': np.float64(0.46667892571439185),
 'training/train_loss_std': np.float64(0.7008666886359489)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.51002917208286),
 'mean_qoe': np.float64(0.0009595806749112468),
 'time/evaluation': 237.089262008667,
 'total_qoe': np.float64(4.51002917208286)}
Step 0 - mean train loss  1.864366
Step 100 - mean train loss  0.661982
Step 200 - mean train loss  0.602778
Step 300 - mean train loss  0.576078
Step 400 - mean train loss  0.543819
Step 500 - mean train loss  0.500828
Step 600 - mean train loss  0.506599
Step 700 - mean train loss  0.515505
Step 800 - mean train loss  0.512583
Step 900 - mean train loss  0.506514
Epoch 205 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #205 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.7042465209961,
 'training/train_loss_mean': np.float64(0.5013599113169561),
 'training/train_loss_std': np.float64(0.7107833400779919)}
Step 0 - mean train loss  1.543295
Step 100 - mean train loss  0.522046
Step 200 - mean train loss  0.477143
Step 300 - mean train loss  0.496628
Step 400 - mean train loss  0.484389
Step 500 - mean train loss  0.481715
Step 600 - mean train loss  0.471602
Step 700 - mean train loss  0.468628
Step 800 - mean train loss  0.458993
Step 900 - mean train loss  0.463899
Epoch 206 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #206 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.90056538581848,
 'training/train_loss_mean': np.float64(0.46891012960179296),
 'training/train_loss_std': np.float64(0.7166965970952027)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.511421535912932),
 'mean_qoe': np.float64(0.0009598769225346664),
 'time/evaluation': 234.5760772228241,
 'total_qoe': np.float64(4.511421535912932)}
Step 0 - mean train loss  1.564029
Step 100 - mean train loss  0.673673
Step 200 - mean train loss  0.612103
Step 300 - mean train loss  0.580094
Step 400 - mean train loss  0.543951
Step 500 - mean train loss  0.498590
Step 600 - mean train loss  0.501446
Step 700 - mean train loss  0.506295
Step 800 - mean train loss  0.502014
Step 900 - mean train loss  0.495336
Epoch 207 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #207 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.9146785736084,
 'training/train_loss_mean': np.float64(0.488347218151504),
 'training/train_loss_std': np.float64(0.7042374476471109)}
Step 0 - mean train loss  1.529993
Step 100 - mean train loss  0.509215
Step 200 - mean train loss  0.466815
Step 300 - mean train loss  0.481474
Step 400 - mean train loss  0.463221
Step 500 - mean train loss  0.461234
Step 600 - mean train loss  0.451432
Step 700 - mean train loss  0.448667
Step 800 - mean train loss  0.439095
Step 900 - mean train loss  0.443764
Epoch 208 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #208 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.09430265426636,
 'training/train_loss_mean': np.float64(0.4467011577521169),
 'training/train_loss_std': np.float64(0.6892536774733731)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5093932023216),
 'mean_qoe': np.float64(0.000959445362196085),
 'time/evaluation': 234.31191444396973,
 'total_qoe': np.float64(4.5093932023216)}
Step 0 - mean train loss  1.271275
Step 100 - mean train loss  0.603024
Step 200 - mean train loss  0.558361
Step 300 - mean train loss  0.539983
Step 400 - mean train loss  0.511542
Step 500 - mean train loss  0.468105
Step 600 - mean train loss  0.467827
Step 700 - mean train loss  0.474527
Step 800 - mean train loss  0.474139
Step 900 - mean train loss  0.472147
Epoch 209 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #209 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.87730717658997,
 'training/train_loss_mean': np.float64(0.4642201260941389),
 'training/train_loss_std': np.float64(0.6740977837024507)}
Step 0 - mean train loss  1.538999
Step 100 - mean train loss  0.503265
Step 200 - mean train loss  0.467304
Step 300 - mean train loss  0.484635
Step 400 - mean train loss  0.465346
Step 500 - mean train loss  0.457865
Step 600 - mean train loss  0.448329
Step 700 - mean train loss  0.444088
Step 800 - mean train loss  0.434279
Step 900 - mean train loss  0.438045
Epoch 210 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #210 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.77717423439026,
 'training/train_loss_mean': np.float64(0.44226202571910994),
 'training/train_loss_std': np.float64(0.6921576177313659)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.515376267477556),
 'mean_qoe': np.float64(0.0009607183547824588),
 'time/evaluation': 234.40670895576477,
 'total_qoe': np.float64(4.515376267477556)}
Step 0 - mean train loss  1.099773
Step 100 - mean train loss  0.564048
Step 200 - mean train loss  0.525950
Step 300 - mean train loss  0.522202
Step 400 - mean train loss  0.497621
Step 500 - mean train loss  0.456005
Step 600 - mean train loss  0.461280
Step 700 - mean train loss  0.470839
Step 800 - mean train loss  0.466068
Step 900 - mean train loss  0.462633
Epoch 211 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #211 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.8953959941864,
 'training/train_loss_mean': np.float64(0.46133148964946874),
 'training/train_loss_std': np.float64(0.6632333298880708)}
Step 0 - mean train loss  1.448185
Step 100 - mean train loss  0.476104
Step 200 - mean train loss  0.438358
Step 300 - mean train loss  0.438756
Step 400 - mean train loss  0.425560
Step 500 - mean train loss  0.432463
Step 600 - mean train loss  0.421317
Step 700 - mean train loss  0.418229
Step 800 - mean train loss  0.409893
Step 900 - mean train loss  0.412790
Epoch 212 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #212 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.83427739143372,
 'training/train_loss_mean': np.float64(0.41990171704733675),
 'training/train_loss_std': np.float64(0.6594072090478783)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.514136264440374),
 'mean_qoe': np.float64(0.0009604545243490158),
 'time/evaluation': 234.32476687431335,
 'total_qoe': np.float64(4.514136264440374)}
Step 0 - mean train loss  0.887080
Step 100 - mean train loss  0.532808
Step 200 - mean train loss  0.504081
Step 300 - mean train loss  0.498004
Step 400 - mean train loss  0.482334
Step 500 - mean train loss  0.441230
Step 600 - mean train loss  0.445093
Step 700 - mean train loss  0.458325
Step 800 - mean train loss  0.454027
Step 900 - mean train loss  0.448551
Epoch 213 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #213 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.60744571685791,
 'training/train_loss_mean': np.float64(0.4408768323866157),
 'training/train_loss_std': np.float64(0.6534924859068214)}
Step 0 - mean train loss  1.437060
Step 100 - mean train loss  0.452308
Step 200 - mean train loss  0.422525
Step 300 - mean train loss  0.411089
Step 400 - mean train loss  0.388038
Step 500 - mean train loss  0.391008
Step 600 - mean train loss  0.383056
Step 700 - mean train loss  0.381807
Step 800 - mean train loss  0.374616
Step 900 - mean train loss  0.380945
Epoch 214 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #214 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.67857384681702,
 'training/train_loss_mean': np.float64(0.3902176120604543),
 'training/train_loss_std': np.float64(0.6222926420220204)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.520178161167769),
 'mean_qoe': np.float64(0.0009617400342910148),
 'time/evaluation': 236.8910403251648,
 'total_qoe': np.float64(4.520178161167769)}
Step 0 - mean train loss  0.842555
Step 100 - mean train loss  0.491267
Step 200 - mean train loss  0.455069
Step 300 - mean train loss  0.453746
Step 400 - mean train loss  0.445933
Step 500 - mean train loss  0.410293
Step 600 - mean train loss  0.409032
Step 700 - mean train loss  0.418319
Step 800 - mean train loss  0.415785
Step 900 - mean train loss  0.414140
Epoch 215 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #215 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.47810459136963,
 'training/train_loss_mean': np.float64(0.40879513381963695),
 'training/train_loss_std': np.float64(0.6200370733388885)}
Step 0 - mean train loss  1.310684
Step 100 - mean train loss  0.439251
Step 200 - mean train loss  0.400805
Step 300 - mean train loss  0.401143
Step 400 - mean train loss  0.393494
Step 500 - mean train loss  0.394301
Step 600 - mean train loss  0.381206
Step 700 - mean train loss  0.377777
Step 800 - mean train loss  0.366212
Step 900 - mean train loss  0.371610
Epoch 216 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #216 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.23579335212708,
 'training/train_loss_mean': np.float64(0.37850422825024277),
 'training/train_loss_std': np.float64(0.616167366675019)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513448938524479),
 'mean_qoe': np.float64(0.0009603082847924423),
 'time/evaluation': 233.11909937858582,
 'total_qoe': np.float64(4.513448938524479)}
Step 0 - mean train loss  0.927580
Step 100 - mean train loss  0.463711
Step 200 - mean train loss  0.413432
Step 300 - mean train loss  0.399391
Step 400 - mean train loss  0.385760
Step 500 - mean train loss  0.353140
Step 600 - mean train loss  0.354611
Step 700 - mean train loss  0.363601
Step 800 - mean train loss  0.359990
Step 900 - mean train loss  0.360146
Epoch 217 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #217 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.35083532333374,
 'training/train_loss_mean': np.float64(0.3577483881593967),
 'training/train_loss_std': np.float64(0.5619092602468216)}
Step 0 - mean train loss  1.304228
Step 100 - mean train loss  0.421670
Step 200 - mean train loss  0.373940
Step 300 - mean train loss  0.368448
Step 400 - mean train loss  0.363500
Step 500 - mean train loss  0.368819
Step 600 - mean train loss  0.356077
Step 700 - mean train loss  0.353583
Step 800 - mean train loss  0.342246
Step 900 - mean train loss  0.346025
Epoch 218 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #218 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.31770038604736,
 'training/train_loss_mean': np.float64(0.3521660373268183),
 'training/train_loss_std': np.float64(0.590249083882949)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.508757732123045),
 'mean_qoe': np.float64(0.0009593101557708606),
 'time/evaluation': 235.85476088523865,
 'total_qoe': np.float64(4.508757732123045)}
Step 0 - mean train loss  0.909590
Step 100 - mean train loss  0.478478
Step 200 - mean train loss  0.418541
Step 300 - mean train loss  0.398883
Step 400 - mean train loss  0.377476
Step 500 - mean train loss  0.343922
Step 600 - mean train loss  0.345475
Step 700 - mean train loss  0.352949
Step 800 - mean train loss  0.349483
Step 900 - mean train loss  0.344100
Epoch 219 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #219 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.1631178855896,
 'training/train_loss_mean': np.float64(0.341053713386401),
 'training/train_loss_std': np.float64(0.5467620245183755)}
Step 0 - mean train loss  0.924574
Step 100 - mean train loss  0.402439
Step 200 - mean train loss  0.355203
Step 300 - mean train loss  0.333690
Step 400 - mean train loss  0.324125
Step 500 - mean train loss  0.333074
Step 600 - mean train loss  0.323371
Step 700 - mean train loss  0.321307
Step 800 - mean train loss  0.310896
Step 900 - mean train loss  0.314730
Epoch 220 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #220 ====================
>>>>>>>>>> Training Information:
{'time/training': 111.59249234199524,
 'training/train_loss_mean': np.float64(0.32039227478300203),
 'training/train_loss_std': np.float64(0.5370988001523604)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.511969929833813),
 'mean_qoe': np.float64(0.0009599936020923006),
 'time/evaluation': 234.3210027217865,
 'total_qoe': np.float64(4.511969929833813)}
Step 0 - mean train loss  0.779911
Step 100 - mean train loss  0.405831
Step 200 - mean train loss  0.361153
Step 300 - mean train loss  0.353685
Step 400 - mean train loss  0.341931
Step 500 - mean train loss  0.314286
Step 600 - mean train loss  0.312765
Step 700 - mean train loss  0.322370
Step 800 - mean train loss  0.319456
Step 900 - mean train loss  0.315404
Epoch 221 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #221 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.24448680877686,
 'training/train_loss_mean': np.float64(0.31216890465351255),
 'training/train_loss_std': np.float64(0.5105024131232759)}
Step 0 - mean train loss  1.021795
Step 100 - mean train loss  0.382158
Step 200 - mean train loss  0.333020
Step 300 - mean train loss  0.311181
Step 400 - mean train loss  0.300555
Step 500 - mean train loss  0.311685
Step 600 - mean train loss  0.300894
Step 700 - mean train loss  0.300071
Step 800 - mean train loss  0.291257
Step 900 - mean train loss  0.297220
Epoch 222 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #222 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.50612306594849,
 'training/train_loss_mean': np.float64(0.3021624436774055),
 'training/train_loss_std': np.float64(0.5174274319764317)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.509460710509706),
 'mean_qoe': np.float64(0.000959459725640363),
 'time/evaluation': 231.6821928024292,
 'total_qoe': np.float64(4.509460710509706)}
Step 0 - mean train loss  0.728877
Step 100 - mean train loss  0.379516
Step 200 - mean train loss  0.329964
Step 300 - mean train loss  0.322386
Step 400 - mean train loss  0.311922
Step 500 - mean train loss  0.288732
Step 600 - mean train loss  0.290319
Step 700 - mean train loss  0.299807
Step 800 - mean train loss  0.296207
Step 900 - mean train loss  0.293428
Epoch 223 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #223 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.4037401676178,
 'training/train_loss_mean': np.float64(0.29109567025953803),
 'training/train_loss_std': np.float64(0.48856492584327854)}
Step 0 - mean train loss  0.948681
Step 100 - mean train loss  0.374039
Step 200 - mean train loss  0.317600
Step 300 - mean train loss  0.296236
Step 400 - mean train loss  0.283814
Step 500 - mean train loss  0.295158
Step 600 - mean train loss  0.283291
Step 700 - mean train loss  0.279327
Step 800 - mean train loss  0.271021
Step 900 - mean train loss  0.276875
Epoch 224 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #224 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.96991682052612,
 'training/train_loss_mean': np.float64(0.2832141467689019),
 'training/train_loss_std': np.float64(0.5017875512972163)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.512568313365738),
 'mean_qoe': np.float64(0.000960120917737391),
 'time/evaluation': 233.86843729019165,
 'total_qoe': np.float64(4.512568313365738)}
Step 0 - mean train loss  0.614572
Step 100 - mean train loss  0.359606
Step 200 - mean train loss  0.313081
Step 300 - mean train loss  0.305647
Step 400 - mean train loss  0.299317
Step 500 - mean train loss  0.276919
Step 600 - mean train loss  0.277853
Step 700 - mean train loss  0.288180
Step 800 - mean train loss  0.283758
Step 900 - mean train loss  0.280025
Epoch 225 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #225 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.8209331035614,
 'training/train_loss_mean': np.float64(0.2766621239689181),
 'training/train_loss_std': np.float64(0.4811924069117162)}
Step 0 - mean train loss  0.625047
Step 100 - mean train loss  0.330613
Step 200 - mean train loss  0.284316
Step 300 - mean train loss  0.258183
Step 400 - mean train loss  0.246810
Step 500 - mean train loss  0.255244
Step 600 - mean train loss  0.245953
Step 700 - mean train loss  0.244581
Step 800 - mean train loss  0.238231
Step 900 - mean train loss  0.244605
Epoch 226 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #226 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.83001565933228,
 'training/train_loss_mean': np.float64(0.25116107006945515),
 'training/train_loss_std': np.float64(0.4586618254135597)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5148565755880306),
 'mean_qoe': np.float64(0.0009606077820400065),
 'time/evaluation': 234.3217010498047,
 'total_qoe': np.float64(4.5148565755880306)}
Step 0 - mean train loss  0.499197
Step 100 - mean train loss  0.333089
Step 200 - mean train loss  0.292656
Step 300 - mean train loss  0.278535
Step 400 - mean train loss  0.273623
Step 500 - mean train loss  0.252688
Step 600 - mean train loss  0.255511
Step 700 - mean train loss  0.265646
Step 800 - mean train loss  0.260917
Step 900 - mean train loss  0.257425
Epoch 227 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #227 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.63899397850037,
 'training/train_loss_mean': np.float64(0.2534995338695736),
 'training/train_loss_std': np.float64(0.4562241755550259)}
Step 0 - mean train loss  0.665357
Step 100 - mean train loss  0.309651
Step 200 - mean train loss  0.250855
Step 300 - mean train loss  0.228161
Step 400 - mean train loss  0.215417
Step 500 - mean train loss  0.222922
Step 600 - mean train loss  0.214968
Step 700 - mean train loss  0.215238
Step 800 - mean train loss  0.209453
Step 900 - mean train loss  0.216293
Epoch 228 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #228 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.77595400810242,
 'training/train_loss_mean': np.float64(0.22180069565160965),
 'training/train_loss_std': np.float64(0.4229286613497696)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.516513410512602),
 'mean_qoe': np.float64(0.0009609603001090643),
 'time/evaluation': 233.9663441181183,
 'total_qoe': np.float64(4.516513410512602)}
Step 0 - mean train loss  0.599949
Step 100 - mean train loss  0.294026
Step 200 - mean train loss  0.262517
Step 300 - mean train loss  0.242249
Step 400 - mean train loss  0.234449
Step 500 - mean train loss  0.219441
Step 600 - mean train loss  0.225612
Step 700 - mean train loss  0.233926
Step 800 - mean train loss  0.230525
Step 900 - mean train loss  0.227587
Epoch 229 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #229 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.8586072921753,
 'training/train_loss_mean': np.float64(0.22428260991138274),
 'training/train_loss_std': np.float64(0.42060233309795536)}
Step 0 - mean train loss  0.788418
Step 100 - mean train loss  0.290008
Step 200 - mean train loss  0.229157
Step 300 - mean train loss  0.204396
Step 400 - mean train loss  0.194215
Step 500 - mean train loss  0.201598
Step 600 - mean train loss  0.193995
Step 700 - mean train loss  0.193096
Step 800 - mean train loss  0.186706
Step 900 - mean train loss  0.191524
Epoch 230 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #230 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.91209816932678,
 'training/train_loss_mean': np.float64(0.19542751779696815),
 'training/train_loss_std': np.float64(0.3922704336410297)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513497138873059),
 'mean_qoe': np.float64(0.0009603185401857572),
 'time/evaluation': 232.29167866706848,
 'total_qoe': np.float64(4.513497138873059)}
Step 0 - mean train loss  0.514296
Step 100 - mean train loss  0.241069
Step 200 - mean train loss  0.220585
Step 300 - mean train loss  0.206027
Step 400 - mean train loss  0.197036
Step 500 - mean train loss  0.183212
Step 600 - mean train loss  0.186596
Step 700 - mean train loss  0.195487
Step 800 - mean train loss  0.193390
Step 900 - mean train loss  0.188473
Epoch 231 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #231 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.63834619522095,
 'training/train_loss_mean': np.float64(0.18574384930036514),
 'training/train_loss_std': np.float64(0.364111675082008)}
Step 0 - mean train loss  0.643789
Step 100 - mean train loss  0.252739
Step 200 - mean train loss  0.202350
Step 300 - mean train loss  0.173207
Step 400 - mean train loss  0.161276
Step 500 - mean train loss  0.167994
Step 600 - mean train loss  0.161740
Step 700 - mean train loss  0.160773
Step 800 - mean train loss  0.155437
Step 900 - mean train loss  0.159478
Epoch 232 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #232 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.11178827285767,
 'training/train_loss_mean': np.float64(0.1622498069722027),
 'training/train_loss_std': np.float64(0.34129323900797676)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.513788373222737),
 'mean_qoe': np.float64(0.0009603805049410079),
 'time/evaluation': 234.4187936782837,
 'total_qoe': np.float64(4.513788373222737)}
Step 0 - mean train loss  0.257689
Step 100 - mean train loss  0.193638
Step 200 - mean train loss  0.175219
Step 300 - mean train loss  0.164371
Step 400 - mean train loss  0.155997
Step 500 - mean train loss  0.144512
Step 600 - mean train loss  0.143515
Step 700 - mean train loss  0.150371
Step 800 - mean train loss  0.149362
Step 900 - mean train loss  0.145133
Epoch 233 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #233 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.7648766040802,
 'training/train_loss_mean': np.float64(0.14182145603905816),
 'training/train_loss_std': np.float64(0.29188500070723233)}
Step 0 - mean train loss  0.499710
Step 100 - mean train loss  0.192420
Step 200 - mean train loss  0.145904
Step 300 - mean train loss  0.123646
Step 400 - mean train loss  0.114199
Step 500 - mean train loss  0.121698
Step 600 - mean train loss  0.116810
Step 700 - mean train loss  0.117668
Step 800 - mean train loss  0.114041
Step 900 - mean train loss  0.117788
Epoch 234 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #234 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.80052709579468,
 'training/train_loss_mean': np.float64(0.11942064047332224),
 'training/train_loss_std': np.float64(0.2715496764702977)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.515824095309269),
 'mean_qoe': np.float64(0.0009608136372998445),
 'time/evaluation': 237.20121312141418,
 'total_qoe': np.float64(4.515824095309269)}
Step 0 - mean train loss  0.141504
Step 100 - mean train loss  0.139551
Step 200 - mean train loss  0.124701
Step 300 - mean train loss  0.118954
Step 400 - mean train loss  0.108579
Step 500 - mean train loss  0.101758
Step 600 - mean train loss  0.100219
Step 700 - mean train loss  0.104430
Step 800 - mean train loss  0.104557
Step 900 - mean train loss  0.100904
Epoch 235 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #235 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.18492937088013,
 'training/train_loss_mean': np.float64(0.09792024761147841),
 'training/train_loss_std': np.float64(0.22465437452979573)}
Step 0 - mean train loss  0.182160
Step 100 - mean train loss  0.132581
Step 200 - mean train loss  0.099857
Step 300 - mean train loss  0.080572
Step 400 - mean train loss  0.072345
Step 500 - mean train loss  0.079168
Step 600 - mean train loss  0.076197
Step 700 - mean train loss  0.077489
Step 800 - mean train loss  0.075597
Step 900 - mean train loss  0.077510
Epoch 236 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #236 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.934885263443,
 'training/train_loss_mean': np.float64(0.07759426560941499),
 'training/train_loss_std': np.float64(0.20609686256675752)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.5066160778588635),
 'mean_qoe': np.float64(0.0009588544846508221),
 'time/evaluation': 233.89261865615845,
 'total_qoe': np.float64(4.5066160778588635)}
Step 0 - mean train loss  0.014313
Step 100 - mean train loss  0.083593
Step 200 - mean train loss  0.076236
Step 300 - mean train loss  0.074607
Step 400 - mean train loss  0.065111
Step 500 - mean train loss  0.062362
Step 600 - mean train loss  0.060827
Step 700 - mean train loss  0.063440
Step 800 - mean train loss  0.064627
Step 900 - mean train loss  0.063273
Epoch 237 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #237 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.26418828964233,
 'training/train_loss_mean': np.float64(0.060890412435481774),
 'training/train_loss_std': np.float64(0.17152056317251296)}
Step 0 - mean train loss  0.020881
Step 100 - mean train loss  0.094238
Step 200 - mean train loss  0.067456
Step 300 - mean train loss  0.051660
Step 400 - mean train loss  0.046780
Step 500 - mean train loss  0.050860
Step 600 - mean train loss  0.049118
Step 700 - mean train loss  0.050272
Step 800 - mean train loss  0.048598
Step 900 - mean train loss  0.050178
Epoch 238 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #238 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.96097612380981,
 'training/train_loss_mean': np.float64(0.04981274411334764),
 'training/train_loss_std': np.float64(0.16118325791933108)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.497753068941994),
 'mean_qoe': np.float64(0.0009569687380727647),
 'time/evaluation': 234.2327482700348,
 'total_qoe': np.float64(4.497753068941994)}
Step 0 - mean train loss  0.000216
Step 100 - mean train loss  0.048206
Step 200 - mean train loss  0.044107
Step 300 - mean train loss  0.046313
Step 400 - mean train loss  0.039589
Step 500 - mean train loss  0.038418
Step 600 - mean train loss  0.037859
Step 700 - mean train loss  0.038422
Step 800 - mean train loss  0.039590
Step 900 - mean train loss  0.038922
Epoch 239 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #239 ====================
>>>>>>>>>> Training Information:
{'time/training': 113.90948510169983,
 'training/train_loss_mean': np.float64(0.037410103322554095),
 'training/train_loss_std': np.float64(0.13231374416531858)}
Checkpoint saved at: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/239
Step 0 - mean train loss  0.004095
Step 100 - mean train loss  0.065802
Step 200 - mean train loss  0.045065
Step 300 - mean train loss  0.033963
Step 400 - mean train loss  0.031206
Step 500 - mean train loss  0.034353
Step 600 - mean train loss  0.033285
Step 700 - mean train loss  0.034332
Step 800 - mean train loss  0.033057
Step 900 - mean train loss  0.035217
Epoch 240 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #240 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.03588533401489,
 'training/train_loss_mean': np.float64(0.03469374922168577),
 'training/train_loss_std': np.float64(0.13078642651714595)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.479047898380202),
 'mean_qoe': np.float64(0.0009529889145489791),
 'time/evaluation': 235.02580189704895,
 'total_qoe': np.float64(4.479047898380202)}
Step 0 - mean train loss  0.000115
Step 100 - mean train loss  0.032727
Step 200 - mean train loss  0.030303
Step 300 - mean train loss  0.034234
Step 400 - mean train loss  0.027715
Step 500 - mean train loss  0.026641
Step 600 - mean train loss  0.026660
Step 700 - mean train loss  0.027547
Step 800 - mean train loss  0.028792
Step 900 - mean train loss  0.028515
Epoch 241 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #241 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.19319200515747,
 'training/train_loss_mean': np.float64(0.027356546354069332),
 'training/train_loss_std': np.float64(0.11420325476911791)}
Step 0 - mean train loss  0.002627
Step 100 - mean train loss  0.052639
Step 200 - mean train loss  0.035168
Step 300 - mean train loss  0.025316
Step 400 - mean train loss  0.023277
Step 500 - mean train loss  0.026144
Step 600 - mean train loss  0.024972
Step 700 - mean train loss  0.025827
Step 800 - mean train loss  0.025103
Step 900 - mean train loss  0.027290
Epoch 242 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #242 ====================
>>>>>>>>>> Training Information:
{'time/training': 114.20619773864746,
 'training/train_loss_mean': np.float64(0.026740279034122953),
 'training/train_loss_std': np.float64(0.11285512849218277)}
>>>>>>>>>> Evaluation Information
{'best_return': np.float64(4.525194102161407),
 'episodes_len': 4700,
 'episodes_return': np.float64(4.47484351410554),
 'mean_qoe': np.float64(0.0009520943647033064),
 'time/evaluation': 238.32643842697144,
 'total_qoe': np.float64(4.47484351410554)}
Step 0 - mean train loss  0.000061
Step 100 - mean train loss  0.026543
Step 200 - mean train loss  0.024058
Step 300 - mean train loss  0.027693
Step 400 - mean train loss  0.021870
Step 500 - mean train loss  0.020784
Step 600 - mean train loss  0.021126
Step 700 - mean train loss  0.021912
Step 800 - mean train loss  0.023355
Step 900 - mean train loss  0.022938
Epoch 243 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #243 ====================
>>>>>>>>>> Training Information:
{'time/training': 115.90405249595642,
 'training/train_loss_mean': np.float64(0.022138095088430107),
 'training/train_loss_std': np.float64(0.10141020176999638)}
Step 0 - mean train loss  0.002898
Step 100 - mean train loss  0.045370
Step 200 - mean train loss  0.030023
Step 300 - mean train loss  0.021008
Step 400 - mean train loss  0.019183
Step 500 - mean train loss  0.021818
Step 600 - mean train loss  0.020682
Step 700 - mean train loss  0.021355
Step 800 - mean train loss  0.020700
Step 900 - mean train loss  0.023213
Epoch 244 losses appended to: data/ft_plms/llama_base/artifacts_exp_pools_ss_None/abrllm_rank_128_w_20_gamma_1.0_sfd_512_sattn_True_sahd_2048_fusion_weighted_sum_lr_5e-05_wd_0.0001_warm_1000_epochs_300_seed_666/checkpoint/train_losses.txt
==================== Training Iteration #244 ====================
>>>>>>>>>> Training Information:
{'time/training': 116.5601167678833,
 'training/train_loss_mean': np.float64(0.022543635229756623),
 'training/train_loss_std': np.float64(0.10428550375760737)}
